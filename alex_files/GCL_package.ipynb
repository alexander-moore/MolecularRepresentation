{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5eeb1cdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import GCL\n",
    "#from GCL.examples import InfoGraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "7ec077ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([99, 19])\n",
      "in training,  torch.Size([1786, 11]) torch.Size([2, 3720]) torch.Size([99, 19])\n",
      "219.05809020996094\n",
      "torch.Size([99, 19])\n",
      "in training,  torch.Size([1770, 11]) torch.Size([2, 3686]) torch.Size([99, 19])\n",
      "207.98971557617188\n",
      "torch.Size([99, 19])\n",
      "in training,  torch.Size([1788, 11]) torch.Size([2, 3740]) torch.Size([99, 19])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Alex\\anaconda3\\envs\\rdk\\lib\\site-packages\\torch_geometric\\deprecation.py:12: UserWarning: 'dropout_adj' is deprecated, use 'dropout_edge' instead\n",
      "  warnings.warn(out)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "202.4779510498047\n",
      "torch.Size([99, 19])\n",
      "in training,  torch.Size([1806, 11]) torch.Size([2, 3742]) torch.Size([99, 19])\n",
      "223.43893432617188\n",
      "torch.Size([99, 19])\n",
      "in training,  torch.Size([1759, 11]) torch.Size([2, 3694]) torch.Size([99, 19])\n",
      "220.12379455566406\n",
      "torch.Size([99, 19])\n",
      "in training,  torch.Size([1760, 11]) torch.Size([2, 3646]) torch.Size([99, 19])\n",
      "180.06825256347656\n",
      "torch.Size([99, 19])\n",
      "in training,  torch.Size([1772, 11]) torch.Size([2, 3690]) torch.Size([99, 19])\n",
      "198.85830688476562\n",
      "torch.Size([99, 19])\n",
      "in training,  torch.Size([1763, 11]) torch.Size([2, 3626]) torch.Size([99, 19])\n",
      "165.28704833984375\n",
      "torch.Size([99, 19])\n",
      "in training,  torch.Size([1756, 11]) torch.Size([2, 3620]) torch.Size([99, 19])\n",
      "193.99253845214844\n",
      "torch.Size([99, 19])\n",
      "in training,  torch.Size([1805, 11]) torch.Size([2, 3738]) torch.Size([99, 19])\n",
      "160.2373046875\n",
      "torch.Size([99, 19])\n",
      "in training,  torch.Size([1824, 11]) torch.Size([2, 3762]) torch.Size([99, 19])\n",
      "173.57655334472656\n",
      "torch.Size([99, 19])\n",
      "in training,  torch.Size([1769, 11]) torch.Size([2, 3662]) torch.Size([99, 19])\n",
      "165.43072509765625\n",
      "encoding torch.Size([1744, 11]) torch.Size([2, 3598])\n",
      "out of enc torch.Size([1744, 128])\n",
      "torch.Size([1744, 128]) torch.Size([99, 19])\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Found input variables with inconsistent numbers of samples: [1744, 99]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[54], line 181\u001b[0m\n\u001b[0;32m    177\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mscore \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mscore\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m    180\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m--> 181\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[54], line 176\u001b[0m, in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m    173\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m bind \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m10\u001b[39m:\n\u001b[0;32m    174\u001b[0m             \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m--> 176\u001b[0m score \u001b[38;5;241m=\u001b[39m \u001b[43mvalidation\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgconv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    177\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mscore \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mscore\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "Cell \u001b[1;32mIn[54], line 135\u001b[0m, in \u001b[0;36mvalidation\u001b[1;34m(gcn, loader)\u001b[0m\n\u001b[0;32m    132\u001b[0m         rep \u001b[38;5;241m=\u001b[39m rep\u001b[38;5;241m.\u001b[39mcpu()\n\u001b[0;32m    134\u001b[0m         \u001b[38;5;28mprint\u001b[39m(rep\u001b[38;5;241m.\u001b[39mshape, batch\u001b[38;5;241m.\u001b[39my\u001b[38;5;241m.\u001b[39mshape)\n\u001b[1;32m--> 135\u001b[0m         lm \u001b[38;5;241m=\u001b[39m \u001b[43mLinearRegression\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrep\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    136\u001b[0m         score \u001b[38;5;241m=\u001b[39m score \u001b[38;5;241m+\u001b[39m f1_score(y, lm\u001b[38;5;241m.\u001b[39mpredict(rep))\n\u001b[0;32m    138\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m score \u001b[38;5;241m/\u001b[39m (bind\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\rdk\\lib\\site-packages\\sklearn\\linear_model\\_base.py:649\u001b[0m, in \u001b[0;36mLinearRegression.fit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    645\u001b[0m n_jobs_ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_jobs\n\u001b[0;32m    647\u001b[0m accept_sparse \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpositive \u001b[38;5;28;01melse\u001b[39;00m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcsr\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcsc\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcoo\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m--> 649\u001b[0m X, y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_data\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    650\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccept_sparse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maccept_sparse\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_numeric\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmulti_output\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\n\u001b[0;32m    651\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    653\u001b[0m sample_weight \u001b[38;5;241m=\u001b[39m _check_sample_weight(\n\u001b[0;32m    654\u001b[0m     sample_weight, X, dtype\u001b[38;5;241m=\u001b[39mX\u001b[38;5;241m.\u001b[39mdtype, only_non_negative\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m    655\u001b[0m )\n\u001b[0;32m    657\u001b[0m X, y, X_offset, y_offset, X_scale \u001b[38;5;241m=\u001b[39m _preprocess_data(\n\u001b[0;32m    658\u001b[0m     X,\n\u001b[0;32m    659\u001b[0m     y,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    662\u001b[0m     sample_weight\u001b[38;5;241m=\u001b[39msample_weight,\n\u001b[0;32m    663\u001b[0m )\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\rdk\\lib\\site-packages\\sklearn\\base.py:554\u001b[0m, in \u001b[0;36mBaseEstimator._validate_data\u001b[1;34m(self, X, y, reset, validate_separately, **check_params)\u001b[0m\n\u001b[0;32m    552\u001b[0m         y \u001b[38;5;241m=\u001b[39m check_array(y, input_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcheck_y_params)\n\u001b[0;32m    553\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 554\u001b[0m         X, y \u001b[38;5;241m=\u001b[39m check_X_y(X, y, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcheck_params)\n\u001b[0;32m    555\u001b[0m     out \u001b[38;5;241m=\u001b[39m X, y\n\u001b[0;32m    557\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m check_params\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mensure_2d\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mTrue\u001b[39;00m):\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\rdk\\lib\\site-packages\\sklearn\\utils\\validation.py:1122\u001b[0m, in \u001b[0;36mcheck_X_y\u001b[1;34m(X, y, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, estimator)\u001b[0m\n\u001b[0;32m   1104\u001b[0m X \u001b[38;5;241m=\u001b[39m check_array(\n\u001b[0;32m   1105\u001b[0m     X,\n\u001b[0;32m   1106\u001b[0m     accept_sparse\u001b[38;5;241m=\u001b[39maccept_sparse,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1117\u001b[0m     input_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mX\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   1118\u001b[0m )\n\u001b[0;32m   1120\u001b[0m y \u001b[38;5;241m=\u001b[39m _check_y(y, multi_output\u001b[38;5;241m=\u001b[39mmulti_output, y_numeric\u001b[38;5;241m=\u001b[39my_numeric, estimator\u001b[38;5;241m=\u001b[39mestimator)\n\u001b[1;32m-> 1122\u001b[0m \u001b[43mcheck_consistent_length\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1124\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m X, y\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\rdk\\lib\\site-packages\\sklearn\\utils\\validation.py:397\u001b[0m, in \u001b[0;36mcheck_consistent_length\u001b[1;34m(*arrays)\u001b[0m\n\u001b[0;32m    395\u001b[0m uniques \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39munique(lengths)\n\u001b[0;32m    396\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(uniques) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m--> 397\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    398\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFound input variables with inconsistent numbers of samples: \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    399\u001b[0m         \u001b[38;5;241m%\u001b[39m [\u001b[38;5;28mint\u001b[39m(l) \u001b[38;5;28;01mfor\u001b[39;00m l \u001b[38;5;129;01min\u001b[39;00m lengths]\n\u001b[0;32m    400\u001b[0m     )\n",
      "\u001b[1;31mValueError\u001b[0m: Found input variables with inconsistent numbers of samples: [1744, 99]"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import os.path as osp\n",
    "import GCL.losses as L\n",
    "import GCL.augmentors as A\n",
    "import torch_geometric.transforms as T\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from tqdm import tqdm\n",
    "from torch.optim import Adam\n",
    "from GCL.eval import get_split, LREvaluator\n",
    "from GCL.models.contrast_model import WithinEmbedContrast\n",
    "from torch_geometric.nn import GCNConv\n",
    "from torch_geometric.datasets import QM9\n",
    "# from pl_bolts.optimizers import LinearWarmupCosineAnnealingLR\n",
    "from torch_geometric.loader import DataLoader\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "device = 'cuda'\n",
    "\n",
    "class GConv(torch.nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim):\n",
    "        super(GConv, self).__init__()\n",
    "        self.act = torch.nn.PReLU()\n",
    "        self.bn = torch.nn.BatchNorm1d(2 * hidden_dim, momentum=0.01)\n",
    "        self.conv1 = GCNConv(input_dim, 2 * hidden_dim, cached=False)\n",
    "        self.conv2 = GCNConv(2 * hidden_dim, hidden_dim, cached=False)\n",
    "\n",
    "    def forward(self, x, edge_index, edge_weight=None):\n",
    "        \n",
    "        print('into gconv', x.shape, edge_index.shape)\n",
    "        z = self.conv1(x, edge_index, edge_weight)\n",
    "        z = self.bn(z)\n",
    "        z = self.act(z)\n",
    "        \n",
    "        print('out1', z.shape)\n",
    "        z = self.conv2(z, edge_index, edge_weight)\n",
    "        print('out GCONV', z.shape)\n",
    "        return z\n",
    "\n",
    "class GCN(torch.nn.Module):\n",
    "    def __init__(self, n_features):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.rep_dim = 128\n",
    "        #self.emb_dim = 64\n",
    "        \n",
    "        self.conv1 = GCNConv(n_features, self.rep_dim // 2)\n",
    "        self.bn1 = nn.BatchNorm1d(self.rep_dim // 2)\n",
    "        self.a1 = nn.LeakyReLU(0.02)\n",
    "        \n",
    "        self.conv2 = GCNConv(self.rep_dim // 2, self.rep_dim) # To Rep Space\n",
    "        self.bn2 = nn.BatchNorm1d(self.rep_dim)\n",
    "        self.a2 = nn.LeakyReLU(0.02)\n",
    "        \n",
    "        self.conv3 = GCNConv(self.rep_dim, self.rep_dim * 2) # To Emb Space\n",
    "        self.bn3 = nn.BatchNorm1d(self.rep_dim * 2)\n",
    "        \n",
    "        self.fc1 = nn.Linear(self.rep_dim * 2, 999) # Linear to rep?\n",
    "        \n",
    "    def forward(self, x, edge_index, donut):\n",
    "        #x = x.float().to(device)\n",
    "        #edge_index = data[1].to(device)\n",
    "        \n",
    "        #print(x.dtype)\n",
    "        #print(edge_index.dtype)\n",
    "        #x, edge_index = data.x.float(), data.edge_index\n",
    "        \n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = self.a1(self.bn1(x))\n",
    "        x = F.dropout(x, training=self.training)\n",
    "        \n",
    "        x = self.conv2(x, edge_index)\n",
    "        #x = self.a2(self.bn2(x))\n",
    "        #x = F.dropout(x, training=self.training)\n",
    "        x_rep = self.bn2(x)\n",
    "        x_emb = self.conv3(x_rep, edge_index)\n",
    "\n",
    "        # Can have the -> rep and -> emb layers be linear layers on the graph conv output\n",
    "        x_fc1 = self.fc1(x_emb)\n",
    "        #print('from conv3 to linear output', x_fc1.shape)\n",
    "        \n",
    "        return x_rep, x_emb\n",
    "\n",
    "class Encoder(torch.nn.Module):\n",
    "    # Encoder is not itself a model, it holds the augs and forward function of the GConv model\n",
    "    def __init__(self, encoder, augmentor):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.encoder = encoder\n",
    "        self.augmentor = augmentor\n",
    "\n",
    "    def forward(self, x, edge_index, edge_weight=None):\n",
    "        aug1, aug2 = self.augmentor # unpack two augmentations\n",
    "        x1, edge_index1, edge_weight1 = aug1(x, edge_index, edge_weight)\n",
    "        x2, edge_index2, edge_weight2 = aug2(x, edge_index, edge_weight)\n",
    "        \n",
    "        #print('x, x1, x2', x.shape, x1.shape, x2.shape)\n",
    "        \n",
    "        # Encoder passes GConv over each of untransformed x, aug1(x), and aug2(x)\n",
    "        z, _ = self.encoder(x, edge_index, edge_weight)\n",
    "        _, z1 = self.encoder(x1, edge_index1, edge_weight1)\n",
    "        _, z2 = self.encoder(x2, edge_index2, edge_weight2)\n",
    "        \n",
    "        #print('z, z1, z2', z.shape, z1.shape, z2.shape)\n",
    "        return z, z1, z2\n",
    "\n",
    "\n",
    "def train(encoder_model, contrast_model, batch, optimizer):\n",
    "    encoder_model.train()\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    print('in training, ', batch.x.shape, batch.edge_index.shape, batch.y.shape)\n",
    "    _, z1, z2 = encoder_model(batch.x.to(device), batch.edge_index.to(device), batch.edge_attr.to(device))\n",
    "    loss = contrast_model(z1, z2)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    return loss.item()\n",
    "\n",
    "\n",
    "def validation(gcn, loader):\n",
    "    gcn.eval()\n",
    "    score = 0\n",
    "    with torch.no_grad():\n",
    "        for bind, batch in enumerate(loader):\n",
    "            \n",
    "            print('encoding', batch.x.shape, batch.edge_index.shape)\n",
    "            rep, _ = gcn(batch.x.to(device), batch.edge_index.to(device), batch.edge_attr.to(device))\n",
    "            \n",
    "            print('out of enc', rep.shape)\n",
    "            rep = rep.cpu()\n",
    "            \n",
    "            print(rep.shape, batch.y.shape)\n",
    "            lm = LinearRegression().fit(rep, batch.y)\n",
    "            score = score + f1_score(y, lm.predict(rep))\n",
    "                \n",
    "    return score / (bind+1)\n",
    "\n",
    "\n",
    "def main():\n",
    "    device = torch.device('cuda')\n",
    "    path = osp.join(osp.expanduser('~'), 'datasets', 'QM9')\n",
    "\n",
    "    train_dataset = QM9(root = 'datasets/', transform=T.NormalizeFeatures()) # subset false -> 250k graphs\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=99, shuffle=True)\n",
    "    \n",
    "    aug1 = A.Compose([A.EdgeRemoving(pe=0.5), A.FeatureMasking(pf=0.1)])\n",
    "    aug2 = A.Compose([A.EdgeRemoving(pe=0.5), A.FeatureMasking(pf=0.1)])\n",
    "    \n",
    "    #print(train_dataset.num_node_features)\n",
    "    #gconv = GConv(input_dim=train_dataset.num_node_features, hidden_dim=256).to(device)\n",
    "    gconv = GCN(n_features = train_dataset.num_node_features)\n",
    "    encoder_model = Encoder(encoder=gconv, augmentor=(aug1, aug2)).to(device)\n",
    "    contrast_model = WithinEmbedContrast(loss=L.BarlowTwins()).to(device)\n",
    "\n",
    "    optimizer = Adam(encoder_model.parameters(), lr=5e-4)\n",
    "#     scheduler = LinearWarmupCosineAnnealingLR(\n",
    "#         optimizer=optimizer,\n",
    "#         warmup_epochs=400,\n",
    "#         max_epochs=4000)\n",
    "\n",
    "    for epoch in range(1, 2):\n",
    "        for bind, batch in enumerate(train_loader):\n",
    "            \n",
    "            print(batch.y.shape)\n",
    "            loss = train(encoder_model, contrast_model, batch, optimizer)\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "            print(loss)\n",
    "            \n",
    "            if bind > 10:\n",
    "                break\n",
    "            \n",
    "    score = validation(gconv, train_loader)\n",
    "    print(f'score {score}')\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d517f92",
   "metadata": {},
   "source": [
    "    +--------+----------------------------------+-----------------------------------------------------------------------------------+---------------------------------------------+\n",
    "    | Target | Property                         | Description                                                                       | Unit                                        |\n",
    "    | 0      | :math:`\\mu`                      | Dipole moment                                                                     | :math:`\\textrm{D}`                          |\n",
    "    | 1      | :math:`\\alpha`                   | Isotropic polarizability                                                          | :math:`{a_0}^3`                             |\n",
    "    | 2      | :math:`\\epsilon_{\\textrm{HOMO}}` | Highest occupied molecular orbital energy                                         | :math:`\\textrm{eV}`                         |\n",
    "    | 3      | :math:`\\epsilon_{\\textrm{LUMO}}` | Lowest unoccupied molecular orbital energy                                        | :math:`\\textrm{eV}`                         |\n",
    "    | 4      | :math:`\\Delta \\epsilon`          | Gap between :math:`\\epsilon_{\\textrm{HOMO}}` and :math:`\\epsilon_{\\textrm{LUMO}}` | :math:`\\textrm{eV}`                         |\n",
    "    \n",
    "    | 5      | :math:`\\langle R^2 \\rangle`      | Electronic spatial extent                                                         | :math:`{a_0}^2`                             |\n",
    "    | 6      | :math:`\\textrm{ZPVE}`            | Zero point vibrational energy                                                     | :math:`\\textrm{eV}`                         |\n",
    "    | 7      | :math:`U_0`                      | Internal energy at 0K                                                             | :math:`\\textrm{eV}`                         |\n",
    "    | 8      | :math:`U`                        | Internal energy at 298.15K                                                        | :math:`\\textrm{eV}`                         |\n",
    "    | 9      | :math:`H`                        | Enthalpy at 298.15K                                                               | :math:`\\textrm{eV}`                         |\n",
    "    | 10     | :math:`G`                        | Free energy at 298.15K                                                            | :math:`\\textrm{eV}`                         |\n",
    "    | 11     | :math:`c_{\\textrm{v}}`           | Heat capavity at 298.15K                                                          | :math:`\\frac{\\textrm{cal}}{\\textrm{mol K}}` |\n",
    "    | 12     | :math:`U_0^{\\textrm{ATOM}}`      | Atomization energy at 0K                                                          | :math:`\\textrm{eV}`                         |\n",
    "    | 13     | :math:`U^{\\textrm{ATOM}}`        | Atomization energy at 298.15K                                                     | :math:`\\textrm{eV}`                         |\n",
    "    | 14     | :math:`H^{\\textrm{ATOM}}`        | Atomization enthalpy at 298.15K                                                   | :math:`\\textrm{eV}`                         |\n",
    "    | 15     | :math:`G^{\\textrm{ATOM}}`        | Atomization free energy at 298.15K                                                | :math:`\\textrm{eV}`                         |\n",
    "    | 16     | :math:`A`                        | Rotational constant                                                               | :math:`\\textrm{GHz}`                        |\n",
    "    | 17     | :math:`B`                        | Rotational constant                                                               | :math:`\\textrm{GHz}`                        |\n",
    "    | 18     | :math:`C`                        | Rotational constant    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebe920d4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb2bed27",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
