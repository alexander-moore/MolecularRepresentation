{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "99509334",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.13.1+cu117\n"
     ]
    }
   ],
   "source": [
    "# https://arxiv.org/abs/1610.02415\n",
    "\n",
    "# https://pytorch-geometric.readthedocs.io/en/latest/notes/introduction.html\n",
    "\n",
    "import torch\n",
    "print(torch.__version__)\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "import torch.distributed as dist\n",
    "\n",
    "import torch_geometric\n",
    "#print(torch_geometric.__version__)\n",
    "from torch_geometric.datasets import QM9\n",
    "import GCL.augmentors\n",
    "import GCL.augmentors as A\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.linear_model import RidgeClassifierCV, LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "73640a88",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "parameters = {}\n",
    "parameters['batch_size'] = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2545b3b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000 128831\n",
      "<class 'list'> <class 'int'>\n"
     ]
    }
   ],
   "source": [
    "whole_dataset = QM9(root = 'data/')\n",
    "\n",
    "#print(whole_dataset.get_summary())\n",
    "#print(dir(whole_dataset))\n",
    "#print(whole_dataset.len())\n",
    "\n",
    "n = whole_dataset.len()\n",
    "tr_n = 2000 # Number of QM9 to use as training data\n",
    "\n",
    "all_inds = range(n)\n",
    "tr_inds, val_inds = train_test_split(all_inds, train_size = tr_n)\n",
    "print(len(tr_inds), len(val_inds))\n",
    "print(type(tr_inds), type(tr_inds[0]))\n",
    "\n",
    "\n",
    "train_sampler = torch.utils.data.SubsetRandomSampler(tr_inds)\n",
    "val_sampler = torch.utils.data.SubsetRandomSampler(val_inds)\n",
    "\n",
    "# We need to make a train and validation set since QM9 does not provide them\n",
    "train_set = torch.utils.data.Subset(whole_dataset, tr_inds)\n",
    "val_set = torch.utils.data.Subset(whole_dataset, val_inds)\n",
    "\n",
    "train_loader = torch_geometric.loader.DataLoader(train_set, batch_size = parameters['batch_size'],\n",
    "                                                shuffle = True, num_workers = 2,)\n",
    "                                                #sampler = train_sampler)\n",
    "\n",
    "val_loader = torch_geometric.loader.DataLoader(val_set, batch_size=2048,\n",
    "                                            shuffle=False, num_workers=2,)\n",
    "                                              #sampler = val_sampler)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d664ee6c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d03db0f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from torch_geometric.loader import DataLoader\n",
    "# import torch\n",
    "\n",
    "# infinity = int(1e9)\n",
    "\n",
    "# train_loader = DataLoader(train_dataset, batch_size=parameters['batch_size'], shuffle=True)\n",
    "\n",
    "# train_big_subset = DataLoader(train_dataset, batch_size = 4096, shuffle = True)\n",
    "# val_loader = DataLoader(val_dataset, batch_size = infinity, shuffle = False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d554356d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Transforms\n",
    "# Transforms are a common way in torchvision to transform images and perform augmentation. PyG comes with its own transforms,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3c6ad2e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.nn import GCNConv\n",
    "\n",
    "class GCN(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.rep_dim = 128\n",
    "        #self.emb_dim = 64\n",
    "        \n",
    "        self.conv1 = GCNConv(whole_dataset.num_node_features, self.rep_dim // 2)\n",
    "        self.bn1 = nn.BatchNorm1d(self.rep_dim // 2)\n",
    "        self.a1 = nn.LeakyReLU(0.02)\n",
    "        \n",
    "        self.conv2 = GCNConv(self.rep_dim // 2, self.rep_dim) # To Rep Space\n",
    "        self.bn2 = nn.BatchNorm1d(self.rep_dim)\n",
    "        self.a2 = nn.LeakyReLU(0.02)\n",
    "        \n",
    "        self.conv3 = GCNConv(self.rep_dim, self.rep_dim * 2) # To Emb Space\n",
    "        self.bn3 = nn.BatchNorm1d(self.rep_dim * 2)\n",
    "        \n",
    "        self.fc1 = nn.Linear(self.rep_dim * 2, 999) # Linear to rep?\n",
    "        \n",
    "    def forward(self, data):\n",
    "        x = data[0].float().to(device)\n",
    "        edge_index = data[1].to(device)\n",
    "        \n",
    "        #print(x.dtype)\n",
    "        #print(edge_index.dtype)\n",
    "        #x, edge_index = data.x.float(), data.edge_index\n",
    "        \n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = self.a1(self.bn1(x))\n",
    "        x = F.dropout(x, training=self.training)\n",
    "        \n",
    "        x = self.conv2(x, edge_index)\n",
    "        #x = self.a2(self.bn2(x))\n",
    "        #x = F.dropout(x, training=self.training)\n",
    "        x_rep = self.bn2(x)\n",
    "        x_emb = self.conv3(x_rep, edge_index)\n",
    "\n",
    "        # Can have the -> rep and -> emb layers be linear layers on the graph conv output\n",
    "        x_fc1 = self.fc1(x_emb)\n",
    "        #print('from conv3 to linear output', x_fc1.shape)\n",
    "        \n",
    "        return x_rep, x_emb\n",
    "    \n",
    "    def pair_emb_rep(self, x1, x2):\n",
    "        \n",
    "        return self.forward(x1), self.forward(x2)\n",
    "    \n",
    "from sklearn.linear_model import LinearRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b04aa945",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Alex\\anaconda3\\envs\\rdk\\lib\\site-packages\\torch_geometric\\deprecation.py:12: UserWarning: 'dropout_adj' is deprecated, use 'dropout_edge' instead\n",
      "  warnings.warn(out)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "450.62582325935364\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'train_big_subset' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 108\u001b[0m\n\u001b[0;32m    101\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28msum\u001b[39m(epo_losses) \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mlen\u001b[39m(epo_losses))\n\u001b[0;32m    103\u001b[0m \u001b[38;5;66;03m############################\u001b[39;00m\n\u001b[0;32m    104\u001b[0m \u001b[38;5;66;03m## Per-epoch validation step:\u001b[39;00m\n\u001b[0;32m    105\u001b[0m \n\u001b[0;32m    106\u001b[0m \n\u001b[0;32m    107\u001b[0m \u001b[38;5;66;03m# Embed Training Samples:\u001b[39;00m\n\u001b[1;32m--> 108\u001b[0m train_batch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m(\u001b[38;5;28miter\u001b[39m(\u001b[43mtrain_big_subset\u001b[49m))\n\u001b[0;32m    109\u001b[0m \u001b[38;5;66;03m#print('train batch', train_batch)\u001b[39;00m\n\u001b[0;32m    110\u001b[0m train_batch \u001b[38;5;241m=\u001b[39m val_aug(train_batch\u001b[38;5;241m.\u001b[39mx, train_batch\u001b[38;5;241m.\u001b[39medge_index, train_batch\u001b[38;5;241m.\u001b[39medge_attr) \u001b[38;5;66;03m# val_aug is an empty augmentation\u001b[39;00m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'train_big_subset' is not defined"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)\n",
    "model = GCN().to(device)\n",
    "#data = train_dataset[0].to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=5e-4)\n",
    "\n",
    "\n",
    "aug = A.RandomChoice([#A.RWSampling(num_seeds=1000, walk_length=10),\n",
    "                      A.NodeDropping(pn=0.1),\n",
    "                      A.FeatureMasking(pf=0.1),\n",
    "                      A.EdgeRemoving(pe=0.1)],\n",
    "                     num_choices=1)\n",
    "\n",
    "val_aug = A.RandomChoice([], num_choices = 0)\n",
    "\n",
    "\n",
    "def barlow(batch):\n",
    "    # Return two random views of input batch\n",
    "    return aug(batch[0], batch[1]), aug(batch[0], batch[1])\n",
    "\n",
    "def off_diagonal(x):\n",
    "    n, m = x.shape\n",
    "    assert n == m\n",
    "    return x.flatten()[:-1].view(n - 1, n + 1)[:, 1:].flatten()\n",
    "\n",
    "class FullGatherLayer(torch.autograd.Function):\n",
    "    \"\"\"\n",
    "    Gather tensors from all process and support backward propagation\n",
    "    for the gradients across processes.\n",
    "    \"\"\"\n",
    "\n",
    "    @staticmethod\n",
    "    def forward(ctx, x):\n",
    "        output = [torch.zeros_like(x) for _ in range(dist.get_world_size())]\n",
    "        dist.all_gather(output, x)\n",
    "        return tuple(output)\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, *grads):\n",
    "        all_gradients = torch.stack(grads)\n",
    "        dist.all_reduce(all_gradients)\n",
    "        return all_gradients[dist.get_rank()]\n",
    "    \n",
    "def VicRegLoss(x, y):\n",
    "    # https://github.com/facebookresearch/vicreg/blob/4e12602fd495af83efd1631fbe82523e6db092e0/main_vicreg.py#L184\n",
    "    # x, y are output of projector(backbone(x and y))\n",
    "    repr_loss = F.mse_loss(x, y)\n",
    "\n",
    "    x = x - x.mean(dim=0)\n",
    "    y = y - y.mean(dim=0)\n",
    "\n",
    "    std_x = torch.sqrt(x.var(dim=0) + 0.0001)\n",
    "    std_y = torch.sqrt(y.var(dim=0) + 0.0001)\n",
    "    std_loss = torch.mean(F.relu(1 - std_x)) / 2 + torch.mean(F.relu(1 - std_y)) / 2\n",
    "\n",
    "    cov_x = (x.T @ x) / (parameters['batch_size'] - 1)\n",
    "    cov_y = (y.T @ y) / (parameters['batch_size'] - 1)\n",
    "    cov_loss = off_diagonal(cov_x).pow_(2).sum().div(\n",
    "        x.shape[1]\n",
    "    ) + off_diagonal(cov_y).pow_(2).sum().div(x.shape[1])\n",
    "    \n",
    "    # self.num_features -> rep_dim?\n",
    "    loss = (\n",
    "        sim_coeff * repr_loss\n",
    "        + std_coeff * std_loss\n",
    "        + cov_coeff * cov_loss\n",
    "    )\n",
    "    return loss\n",
    "\n",
    "sim_coeff = 25\n",
    "std_coeff = 25\n",
    "cov_coeff = 1\n",
    "\n",
    "model.train()\n",
    "for epoch in range(5):\n",
    "    \n",
    "    epo_losses = []\n",
    "    for batch in train_loader:\n",
    "        #batch = batch.to(device)\n",
    "        batch.x = batch.x.float()#.to(device)\n",
    "        #batch.edge_index = batch.edge_index.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Barlow - get 2 random views of batch\n",
    "        b1 = aug(batch.x, batch.edge_index, batch.edge_attr)\n",
    "        b2 = aug(batch.x, batch.edge_index, batch.edge_attr)\n",
    "        \n",
    "                \n",
    "        # Embed each batch (ignoring representations)\n",
    "        [r1, e1], [r2, e2] = model.pair_emb_rep(b1, b2)\n",
    "\n",
    "        # VicReg loss on projections\n",
    "        loss = VicRegLoss(e1, e2)\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        epo_losses.append(loss.data.item())\n",
    "        \n",
    "    print(sum(epo_losses) / len(epo_losses))\n",
    "    \n",
    "    ############################\n",
    "    ## Per-epoch validation step:\n",
    "    \n",
    "    GCL.eval\n",
    "\n",
    "\n",
    "    # Embed Training Samples:\n",
    "    train_batch = next(iter(train_big_subset))\n",
    "    #print('train batch', train_batch)\n",
    "    train_batch = val_aug(train_batch.x, train_batch.edge_index, train_batch.edge_attr) # val_aug is an empty augmentation\n",
    "    #print('train_batch augd', train_batch)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        tr_rep, _ = model.forward(train_batch)\n",
    "    #print(tr_rep.shape)\n",
    "\n",
    "    # Train linear model on embedded samples:\n",
    "    ridge_mod = RidgeClassifierCV(cv = 4).fit(tr_rep, y_train)\n",
    "    linear_mod = LogisticRegression(penalty = None).fit(tr_rep, y_train)\n",
    "\n",
    "    # Embed validation samples:\n",
    "    val_batch = next(iter(val_loader))\n",
    "    #print('val batch', val_batch)\n",
    "    val_batch = val_aug(val_batch.x, val_batch.edge_index, val_batch.edge_attr) # val_aug is an empty augmentation\n",
    "    #print('val_batch augd', val_batch)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        val_rep, _ = model.forward(val_batch)\n",
    "    #print(val_rep.shape)\n",
    "\n",
    "    # Test linear model on embedded samples:\n",
    "    ridge_score = f1_score(ridge_mod.predict(val_rep), y_val)\n",
    "    linear_score = f1_score(linear_mod.predict(val_rep), y_val)\n",
    "    \n",
    "    print(f'Classifier Scores at Epoch {epoch}:', round(linear_score, 3), round(ridge_score, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e93024f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "if False: # Update for some downstream? Keep in mind this idea of graph masking\n",
    "    # Evaluate\n",
    "    model.eval()\n",
    "    pred = model(data).argmax(dim=1)\n",
    "    correct = (pred[data.test_mask] == data.y[data.test_mask]).sum()\n",
    "    acc = int(correct) / int(data.test_mask.sum())\n",
    "    print(f'Accuracy: {acc:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb308ba1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ca503f6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
