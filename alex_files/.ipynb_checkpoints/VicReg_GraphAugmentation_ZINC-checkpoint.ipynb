{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "99509334",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.13.1+cu117\n"
     ]
    }
   ],
   "source": [
    "# https://arxiv.org/abs/1610.02415\n",
    "\n",
    "# https://pytorch-geometric.readthedocs.io/en/latest/notes/introduction.html\n",
    "\n",
    "import torch\n",
    "print(torch.__version__)\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "import torch.distributed as dist\n",
    "\n",
    "import torch_geometric\n",
    "import torch_geometric.nn as gnn\n",
    "#print(torch_geometric.__version__)\n",
    "from torch_geometric.datasets import QM9\n",
    "import GCL.augmentors\n",
    "import GCL.augmentors as A\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score, mean_squared_error, r2_score\n",
    "from sklearn.linear_model import RidgeClassifierCV, LogisticRegression, LinearRegression\n",
    "import itertools\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "73640a88",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "parameters = {}\n",
    "parameters['batch_size'] = 4096"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2545b3b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000 128831\n",
      "<class 'list'> <class 'int'>\n"
     ]
    }
   ],
   "source": [
    "whole_dataset = QM9(root = 'data/')\n",
    "\n",
    "#print(whole_dataset.get_summary())\n",
    "#print(dir(whole_dataset))\n",
    "#print(whole_dataset.len())\n",
    "\n",
    "n = whole_dataset.len()\n",
    "tr_n = 2000 # Number of QM9 to use as training data\n",
    "\n",
    "all_inds = range(n)\n",
    "tr_inds, val_inds = train_test_split(all_inds, train_size = tr_n)\n",
    "print(len(tr_inds), len(val_inds))\n",
    "print(type(tr_inds), type(tr_inds[0]))\n",
    "\n",
    "\n",
    "train_sampler = torch.utils.data.SubsetRandomSampler(tr_inds)\n",
    "val_sampler = torch.utils.data.SubsetRandomSampler(val_inds)\n",
    "\n",
    "# We need to make a train and validation set since QM9 does not provide them\n",
    "train_set = torch.utils.data.Subset(whole_dataset, tr_inds)\n",
    "val_set = torch.utils.data.Subset(whole_dataset, val_inds)\n",
    "\n",
    "train_loader = torch_geometric.loader.DataLoader(train_set, batch_size = parameters['batch_size'],\n",
    "                                                shuffle = True, num_workers = 2,)\n",
    "                                                #sampler = train_sampler)\n",
    "big_train_loader = torch_geometric.loader.DataLoader(train_set, batch_size = int(1e9),\n",
    "                                                shuffle = True, num_workers = 2,)\n",
    "\n",
    "val_loader = torch_geometric.loader.DataLoader(val_set, batch_size=2048,\n",
    "                                            shuffle=False, num_workers=2,)\n",
    "                                              #sampler = val_sampler)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3890ce46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_items([(0, 'Dipole moment'), (1, 'Isotropic polarizability'), (2, 'Highest occupied molecular orbital energy'), (3, 'Lowest unoccupied molecular orbital energy'), (4, 'Gap between previous 2'), (5, 'Electronic spatial extent'), (6, 'Zero point vibrational energy'), (7, 'Internal energy at 0K'), (8, 'Internal energy at 298.15K'), (9, 'Enthalpy at 298.15K'), (10, 'Free energy at 298.15K'), (11, 'Heat capavity at 298.15K'), (12, 'Atomization energy at 0K'), (13, 'Atomization energy at 298.15K'), (14, 'Atomization enthalpy at 298.15K'), (15, 'Atomization free energy at 298.15K'), (16, 'Rotational constant A'), (17, 'Rotational constant B'), (18, 'Rotational constant C')])\n"
     ]
    }
   ],
   "source": [
    "qm9_index = {0: 'Dipole moment',\n",
    "1: 'Isotropic polarizability',\n",
    "2: 'Highest occupied molecular orbital energy',\n",
    "3: 'Lowest unoccupied molecular orbital energy',\n",
    "4: 'Gap between previous 2',\n",
    "5: 'Electronic spatial extent',\n",
    "6: 'Zero point vibrational energy',\n",
    "7: 'Internal energy at 0K',\n",
    "8: 'Internal energy at 298.15K',\n",
    "9: 'Enthalpy at 298.15K',\n",
    "10: 'Free energy at 298.15K',\n",
    "11: 'Heat capavity at 298.15K',\n",
    "12: 'Atomization energy at 0K',\n",
    "13: 'Atomization energy at 298.15K',\n",
    "14: 'Atomization enthalpy at 298.15K',\n",
    "15: 'Atomization free energy at 298.15K',\n",
    "16: 'Rotational constant A',\n",
    "17: 'Rotational constant B',\n",
    "18: 'Rotational constant C',}\n",
    "\n",
    "print(qm9_index.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f1f707a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here is sample code for how to implement an \"ablation\" of 2-at-a-time augmentations\n",
    "import GCL.augmentors as A\n",
    "aug = A.RandomChoice([#A.RWSampling(num_seeds=1000, walk_length=10),\n",
    "                      A.NodeDropping(pn=0.1),\n",
    "                      A.FeatureMasking(pf=0.1),\n",
    "                      A.EdgeRemoving(pe=0.1)],\n",
    "                     num_choices=1)\n",
    "\n",
    "# From a set of augmentations of length n_augmentations\n",
    "from GCL.augmentors import node_dropping, ppr_diffusion, feature_dropout, edge_adding, rw_sampling\n",
    "aug_set = [node_dropping, ppr_diffusion, feature_dropout, edge_adding, rw_sampling]\n",
    "aug_strs = ['node_dropping', 'ppr_diffusion', 'feature_dropout', 'edge_adding', 'rw_sampling']\n",
    "print(aug_strs)\n",
    "\n",
    "# First get all pairs of indexes on-off in a list of length n_augmentations\n",
    "aug_inds = list(itertools.product([0, 1], repeat=len(aug_set)))\n",
    "aug_inds = [x for x in aug_inds if sum(x)==2]\n",
    "print(aug_inds)\n",
    "\n",
    "# Then for each augmentation, train and test a VicReg model trained under that augment\n",
    "parameters = {}\n",
    "parameters['batch_size'] = 64\n",
    "parameters['learning_rate'] = 0.002\n",
    "# etc parameters here which define model, hparams\n",
    "\n",
    "for aug in aug_inds:\n",
    "\n",
    "    tr_augs = []\n",
    "    for ind, augi in enumerate(aug):\n",
    "        if augi == 1:\n",
    "            tr_augs.append(aug_set[ind])\n",
    "            \n",
    "    print(str(tr_augs[0]).split(' ')[1], str(tr_augs[1]).split(' ')[1])\n",
    "    tr_aug = A.RandomChoice(tr_augs, num_choices = 1)\n",
    "    parameters['train_aug'] = tr_aug\n",
    "    \n",
    "    model, train_loss, val_loss = train_vicreg(parameters)\n",
    "    transfer_scores = transfer_score(model, parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c6ad2e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.nn import GCNConv\n",
    "\n",
    "class GCN(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.rep_dim = 128\n",
    "        self.emb_dim = 256\n",
    "        \n",
    "        # Data under graph\n",
    "        self.conv1 = GCNConv(whole_dataset.num_node_features, self.rep_dim // 2)\n",
    "        self.bn1 = nn.BatchNorm1d(self.rep_dim // 2)\n",
    "        self.a1 = nn.LeakyReLU(0.02)\n",
    "        \n",
    "        self.conv2 = GCNConv(self.rep_dim // 2, self.rep_dim) # To Rep Space\n",
    "        self.bn2 = nn.BatchNorm1d(self.rep_dim)\n",
    "        \n",
    "        # Projection to representation\n",
    "        self.mpool1 = gnn.global_mean_pool\n",
    "        #self.fc1 = nn.Linear(self.rep_dim, self.rep_dim)\n",
    "        \n",
    "        # Graph 2\n",
    "        self.conv3 = GCNConv(self.rep_dim, self.rep_dim * 2) # To Emb Space\n",
    "        self.bn3 = nn.BatchNorm1d(self.rep_dim * 2)\n",
    "        \n",
    "        # Projection to embedding\n",
    "        #self.mpool2 = gnn.global_mean_pool\n",
    "        #self.fc2 = nn.Linear(self.emb_dim, self.emb_dim) # Linear to rep?\n",
    "        \n",
    "    def forward(self, data, binds):\n",
    "        x = data[0].float().to(device)\n",
    "        edge_index = data[1].to(device)\n",
    "        \n",
    "        # Input graph to GConv\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = self.a1(self.bn1(x))\n",
    "        x = F.dropout(x, training=self.training)\n",
    "        \n",
    "        x = self.bn2(self.conv2(x, edge_index))\n",
    "        \n",
    "        # GConv outputs projected to representation space\n",
    "        #print('before pool: ', x.shape)\n",
    "        x_rep = self.mpool1(x, binds)\n",
    "        #print('pooled: ', x_rep.shape)\n",
    "        \n",
    "        #x_rep = self.fc1(x_rep)\n",
    "        #print('projected: ', x_rep.shape, 'gconv', x.shape)\n",
    "        \n",
    "        x_emb = self.conv3(x, edge_index)\n",
    "        #print('x emb after conv3', x_emb.shape)\n",
    "        #x_emb = self.mpool2(x_emb, binds)\n",
    "        #print('after pool', x_emb.shape)\n",
    "        #x_emb = self.fc2(x_emb)\n",
    "        #print('after fc2', x_emb.shape)\n",
    "        \n",
    "        return x_rep, x_emb\n",
    "\n",
    "device = 'cuda'\n",
    "\n",
    "model = GCN().to(device)\n",
    "\n",
    "sim_coeff = 25\n",
    "std_coeff = 25\n",
    "cov_coeff = 1\n",
    "\n",
    "aug = A.RandomChoice([#A.RWSampling(num_seeds=1000, walk_length=10),\n",
    "                      A.NodeDropping(pn=0.1),\n",
    "                      A.FeatureMasking(pf=0.1),\n",
    "                      A.EdgeRemoving(pe=0.1)],\n",
    "                     num_choices=1)\n",
    "val_aug = A.RandomChoice([], num_choices = 0)\n",
    "\n",
    "def off_diagonal(x):\n",
    "    n, m = x.shape\n",
    "    assert n == m\n",
    "    return x.flatten()[:-1].view(n - 1, n + 1)[:, 1:].flatten()\n",
    "\n",
    "def VicRegLoss(x, y):\n",
    "    # https://github.com/facebookresearch/vicreg/blob/4e12602fd495af83efd1631fbe82523e6db092e0/main_vicreg.py#L184\n",
    "    # x, y are output of projector(backbone(x and y))\n",
    "    repr_loss = F.mse_loss(x, y)\n",
    "\n",
    "    x = x - x.mean(dim=0)\n",
    "    y = y - y.mean(dim=0)\n",
    "\n",
    "    std_x = torch.sqrt(x.var(dim=0) + 0.0001)\n",
    "    std_y = torch.sqrt(y.var(dim=0) + 0.0001)\n",
    "    std_loss = torch.mean(F.relu(1 - std_x)) / 2 + torch.mean(F.relu(1 - std_y)) / 2\n",
    "\n",
    "    cov_x = (x.T @ x) / (parameters['batch_size'] - 1)\n",
    "    cov_y = (y.T @ y) / (parameters['batch_size'] - 1)\n",
    "    cov_loss = off_diagonal(cov_x).pow_(2).sum().div(\n",
    "        x.shape[1]\n",
    "    ) + off_diagonal(cov_y).pow_(2).sum().div(x.shape[1])\n",
    "    \n",
    "    # self.num_features -> rep_dim?\n",
    "    loss = (\n",
    "        sim_coeff * repr_loss\n",
    "        + std_coeff * std_loss\n",
    "        + cov_coeff * cov_loss\n",
    "    )\n",
    "    return loss\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.002, weight_decay=5e-4)\n",
    "transfer_mat = torch.zeros((len(qm9_index.keys()), 10))\n",
    "\n",
    "n_epochs = 5\n",
    "tr_losses = []\n",
    "val_losses = []\n",
    "for epoch in range(0,n_epochs+1):\n",
    "    epoch_losses = []\n",
    "    for batch in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        batch_inds = batch.batch.to(device)\n",
    "\n",
    "        # batch of graphs has edge attribs, node attribs - (n_nodes, n_features+1) -> concat (n_nodes, attrib1)\n",
    "\n",
    "        batch.x = batch.x.float()#.to(device)\n",
    "        #batch.edge_index = batch.edge_index.to(device)\n",
    "\n",
    "        # Barlow - get 2 random views of batch\n",
    "        b1 = aug(batch.x, batch.edge_index, batch.edge_attr)\n",
    "        b2 = aug(batch.x, batch.edge_index, batch.edge_attr)\n",
    "\n",
    "        # Embed each batch (ignoring representations)\n",
    "        r1, e1 = model(b1, batch_inds)\n",
    "        r2, e2 = model(b2, batch_inds)\n",
    "\n",
    "        loss = VicRegLoss(e1, e2)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        epoch_losses.append(loss.data.item())\n",
    "        \n",
    "    print('epoch train loss', sum(epoch_losses) / len(epoch_losses))\n",
    "    tr_losses.append(sum(epoch_losses) / len(epoch_losses))\n",
    "    \n",
    "    if epoch % 10 == 0:\n",
    "        \n",
    "        # Downstream supervised loss\n",
    "        for batch in big_train_loader: # take entire train set\n",
    "            with torch.no_grad():\n",
    "                # Embed training set under model\n",
    "                rep_tr, _ = model(val_aug(batch.x, batch.edge_index, batch.edge_attr), batch.batch.to(device))\n",
    "                \n",
    "                \n",
    "                for val_batch in val_loader:\n",
    "                    # Embed validation set under model\n",
    "                    rep_val, _ = model(val_aug(val_batch.x, val_batch.edge_index, val_batch.edge_attr), val_batch.batch.to(device))\n",
    "                    \n",
    "                    # For each task in QM9\n",
    "                    for tar_ind in range(batch.y.shape[1]):\n",
    "                        # Fit a model on model representation of train set\n",
    "\n",
    "                        #print(rep_tr.shape, batch.y[tar_ind].shap)\n",
    "                        lm = LinearRegression().fit(rep_tr.cpu(), batch.y[:,tar_ind])\n",
    "                        # Test the model on model repersentation of val set\n",
    "                        tar_yhat = lm.predict(rep_val.cpu())\n",
    "                        mse_met = mean_squared_error(val_batch.y[:,tar_ind], tar_yhat)\n",
    "                        r2_met = r2_score(val_batch.y[:,tar_ind], tar_yhat)\n",
    "                        #print(qm9_index[tar_ind], mse_met, r2_met)\n",
    "                        transfer_mat[tar_ind, int((epoch-1)//10)] = r2_met\n",
    "        \n",
    "        # VicReg Validation Loss\n",
    "        val_loss = []\n",
    "        for batch in val_loader:\n",
    "            with torch.no_grad():\n",
    "                # VicReg validation loss\n",
    "                b1 = aug(batch.x, batch.edge_index, batch.edge_attr)\n",
    "                b2 = aug(batch.x, batch.edge_index, batch.edge_attr)\n",
    "                r1, e1 = model(b1, batch.batch.to(device))\n",
    "                r2, e2 = model(b2, batch.batch.to(device))\n",
    "                \n",
    "                val_loss.append(VicRegLoss(e1, e2).item())\n",
    "                \n",
    "        val_losses.append(torch.mean(torch.FloatTensor(val_loss)))\n",
    "\n",
    "plt.plot(tr_losses)\n",
    "plt.plot(val_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "605c26e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(transfer_mat, transfer_mat.shape)\n",
    "#print(breaker)\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "for ind, row in enumerate(transfer_mat):\n",
    "    plt.plot(row, label = qm9_index[ind])\n",
    "    \n",
    "plt.legend(loc = 'best')\n",
    "plt.ylim(0,1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b04aa945",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)\n",
    "model = GCN().to(device)\n",
    "#data = train_dataset[0].to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=5e-4)\n",
    "\n",
    "\n",
    "aug = A.RandomChoice([#A.RWSampling(num_seeds=1000, walk_length=10),\n",
    "                      A.NodeDropping(pn=0.1),\n",
    "                      A.FeatureMasking(pf=0.1),\n",
    "                      A.EdgeRemoving(pe=0.1)],\n",
    "                     num_choices=1)\n",
    "\n",
    "val_aug = A.RandomChoice([], num_choices = 0)\n",
    "\n",
    "\n",
    "def barlow(batch):\n",
    "    # Return two random views of input batch\n",
    "    return aug(batch[0], batch[1]), aug(batch[0], batch[1])\n",
    "\n",
    "def off_diagonal(x):\n",
    "    n, m = x.shape\n",
    "    assert n == m\n",
    "    return x.flatten()[:-1].view(n - 1, n + 1)[:, 1:].flatten()\n",
    "\n",
    "class FullGatherLayer(torch.autograd.Function):\n",
    "    \"\"\"\n",
    "    Gather tensors from all process and support backward propagation\n",
    "    for the gradients across processes.\n",
    "    \"\"\"\n",
    "\n",
    "    @staticmethod\n",
    "    def forward(ctx, x):\n",
    "        output = [torch.zeros_like(x) for _ in range(dist.get_world_size())]\n",
    "        dist.all_gather(output, x)\n",
    "        return tuple(output)\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, *grads):\n",
    "        all_gradients = torch.stack(grads)\n",
    "        dist.all_reduce(all_gradients)\n",
    "        return all_gradients[dist.get_rank()]\n",
    "    \n",
    "def VicRegLoss(x, y):\n",
    "    # https://github.com/facebookresearch/vicreg/blob/4e12602fd495af83efd1631fbe82523e6db092e0/main_vicreg.py#L184\n",
    "    # x, y are output of projector(backbone(x and y))\n",
    "    repr_loss = F.mse_loss(x, y)\n",
    "\n",
    "    x = x - x.mean(dim=0)\n",
    "    y = y - y.mean(dim=0)\n",
    "\n",
    "    std_x = torch.sqrt(x.var(dim=0) + 0.0001)\n",
    "    std_y = torch.sqrt(y.var(dim=0) + 0.0001)\n",
    "    std_loss = torch.mean(F.relu(1 - std_x)) / 2 + torch.mean(F.relu(1 - std_y)) / 2\n",
    "\n",
    "    cov_x = (x.T @ x) / (parameters['batch_size'] - 1)\n",
    "    cov_y = (y.T @ y) / (parameters['batch_size'] - 1)\n",
    "    cov_loss = off_diagonal(cov_x).pow_(2).sum().div(\n",
    "        x.shape[1]\n",
    "    ) + off_diagonal(cov_y).pow_(2).sum().div(x.shape[1])\n",
    "    \n",
    "    # self.num_features -> rep_dim?\n",
    "    loss = (\n",
    "        sim_coeff * repr_loss\n",
    "        + std_coeff * std_loss\n",
    "        + cov_coeff * cov_loss\n",
    "    )\n",
    "    return loss\n",
    "\n",
    "sim_coeff = 25\n",
    "std_coeff = 25\n",
    "cov_coeff = 1\n",
    "\n",
    "model.train()\n",
    "for epoch in range(5):\n",
    "    \n",
    "    epo_losses = []\n",
    "    for batch in train_loader:\n",
    "        #batch = batch.to(device)\n",
    "        batch.x = batch.x.float()#.to(device)\n",
    "        #batch.edge_index = batch.edge_index.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Barlow - get 2 random views of batch\n",
    "        b1 = aug(batch.x, batch.edge_index, batch.edge_attr)\n",
    "        b2 = aug(batch.x, batch.edge_index, batch.edge_attr)\n",
    "        \n",
    "                \n",
    "        # Embed each batch (ignoring representations)\n",
    "        [r1, e1], [r2, e2] = model.pair_emb_rep(b1, b2)\n",
    "\n",
    "        # VicReg loss on projections\n",
    "        loss = VicRegLoss(e1, e2)\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        epo_losses.append(loss.data.item())\n",
    "        \n",
    "    print(sum(epo_losses) / len(epo_losses))\n",
    "    \n",
    "    ############################\n",
    "    ## Per-epoch validation step:\n",
    "    \n",
    "    GCL.eval\n",
    "\n",
    "\n",
    "    # Embed Training Samples:\n",
    "    train_batch = next(iter(train_big_subset))\n",
    "    #print('train batch', train_batch)\n",
    "    train_batch = val_aug(train_batch.x, train_batch.edge_index, train_batch.edge_attr) # val_aug is an empty augmentation\n",
    "    #print('train_batch augd', train_batch)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        tr_rep, _ = model.forward(train_batch)\n",
    "    #print(tr_rep.shape)\n",
    "\n",
    "    # Train linear model on embedded samples:\n",
    "    ridge_mod = RidgeClassifierCV(cv = 4).fit(tr_rep, y_train)\n",
    "    linear_mod = LogisticRegression(penalty = None).fit(tr_rep, y_train)\n",
    "\n",
    "    # Embed validation samples:\n",
    "    val_batch = next(iter(val_loader))\n",
    "    #print('val batch', val_batch)\n",
    "    val_batch = val_aug(val_batch.x, val_batch.edge_index, val_batch.edge_attr) # val_aug is an empty augmentation\n",
    "    #print('val_batch augd', val_batch)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        val_rep, _ = model.forward(val_batch)\n",
    "    #print(val_rep.shape)\n",
    "\n",
    "    # Test linear model on embedded samples:\n",
    "    ridge_score = f1_score(ridge_mod.predict(val_rep), y_val)\n",
    "    linear_score = f1_score(linear_mod.predict(val_rep), y_val)\n",
    "    \n",
    "    print(f'Classifier Scores at Epoch {epoch}:', round(linear_score, 3), round(ridge_score, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e93024f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "if False: # Update for some downstream? Keep in mind this idea of graph masking\n",
    "    # Evaluate\n",
    "    model.eval()\n",
    "    pred = model(data).argmax(dim=1)\n",
    "    correct = (pred[data.test_mask] == data.y[data.test_mask]).sum()\n",
    "    acc = int(correct) / int(data.test_mask.sum())\n",
    "    print(f'Accuracy: {acc:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb308ba1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ca503f6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
