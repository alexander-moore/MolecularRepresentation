{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "99509334",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.13.1+cu117\n"
     ]
    }
   ],
   "source": [
    "# https://arxiv.org/abs/1610.02415\n",
    "\n",
    "# https://pytorch-geometric.readthedocs.io/en/latest/notes/introduction.html\n",
    "\n",
    "import torch\n",
    "print(torch.__version__)\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "import torch.distributed as dist\n",
    "\n",
    "import torch_geometric\n",
    "#print(torch_geometric.__version__)\n",
    "from torch_geometric.datasets import ZINC\n",
    "import GCL.augmentors\n",
    "import GCL.augmentors as A\n",
    "\n",
    "from sklearn.linear_model import RidgeClassifierCV, LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "73640a88",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch_geometric.datasets.zinc.ZINC"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch_geometric.datasets.ZINC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "2545b3b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ZINC(10000)\n"
     ]
    }
   ],
   "source": [
    "train_dataset = ZINC(root = 'data/', subset = 'true', split = 'train') # subset false -> 250k graphs\n",
    "                                      # subset true -> 12k graphs\n",
    "val_dataset = ZINC(root = 'data/', split = 'val')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d664ee6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "parameters = {}\n",
    "parameters['batch_size'] = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d03db0f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.loader import DataLoader\n",
    "import torch\n",
    "\n",
    "infinity = int(1e9)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=parameters['batch_size'], shuffle=True)\n",
    "\n",
    "train_big_subset = DataLoader(train_dataset, batch_size = 4096, shuffle = True)\n",
    "val_loader = DataLoader(val_dataset, batch_size = infinity, shuffle = False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d554356d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Transformsïƒ\n",
    "# Transforms are a common way in torchvision to transform images and perform augmentation. PyG comes with its own transforms,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3c6ad2e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.nn import GCNConv\n",
    "\n",
    "class GCN(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.rep_dim = 128\n",
    "        #self.emb_dim = 64\n",
    "        \n",
    "        self.conv1 = GCNConv(train_dataset.num_node_features, self.rep_dim // 2)\n",
    "        self.bn1 = nn.BatchNorm1d(self.rep_dim // 2)\n",
    "        self.a1 = nn.LeakyReLU(0.02)\n",
    "        \n",
    "        self.conv2 = GCNConv(self.rep_dim // 2, self.rep_dim) # To Rep Space\n",
    "        self.bn2 = nn.BatchNorm1d(self.rep_dim)\n",
    "        self.a2 = nn.LeakyReLU(0.02)\n",
    "        \n",
    "        self.conv3 = GCNConv(self.rep_dim, self.rep_dim * 2) # To Emb Space\n",
    "        self.bn3 = nn.BatchNorm1d(self.rep_dim * 2)\n",
    "        \n",
    "        self.fc1 = nn.Linear(self.rep_dim * 2, 999) # Linear to rep?\n",
    "        \n",
    "    def forward(self, data):\n",
    "        x = data[0].float().to(device)\n",
    "        edge_index = data[1].to(device)\n",
    "        \n",
    "        #print(x.dtype)\n",
    "        #print(edge_index.dtype)\n",
    "        #x, edge_index = data.x.float(), data.edge_index\n",
    "        \n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = self.a1(self.bn1(x))\n",
    "        x = F.dropout(x, training=self.training)\n",
    "        \n",
    "        x = self.conv2(x, edge_index)\n",
    "        #x = self.a2(self.bn2(x))\n",
    "        #x = F.dropout(x, training=self.training)\n",
    "        x_rep = self.bn2(x)\n",
    "        x_emb = self.conv3(x_rep, edge_index)\n",
    "\n",
    "        # Can have the -> rep and -> emb layers be linear layers on the graph conv output\n",
    "        x_fc1 = self.fc1(x_emb)\n",
    "        #print('from conv3 to linear output', x_fc1.shape)\n",
    "        \n",
    "        return x_rep, x_emb\n",
    "    \n",
    "    def pair_emb_rep(self, x1, x2):\n",
    "        \n",
    "        return self.forward(x1), self.forward(x2)\n",
    "    \n",
    "from sklearn.linear_model import LinearRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "b04aa945",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n",
      "412.79502799404656\n",
      "validation_batch DataBatch(x=[565429, 1], edge_index=[2, 1216310], edge_attr=[1216310], y=[24445], batch=[565429], ptr=[24446])\n",
      "train_batch (tensor([[0],\n",
      "        [0],\n",
      "        [0],\n",
      "        ...,\n",
      "        [8],\n",
      "        [0],\n",
      "        [0]]), tensor([[    0,     1,     1,  ..., 95152, 95153, 95154],\n",
      "        [    1,     0,     2,  ..., 95154, 95152, 95152]]), tensor([1, 1, 2,  ..., 1, 1, 1]))\n",
      "torch.Size([95155, 128])\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'a' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[36], line 119\u001b[0m\n\u001b[0;32m    116\u001b[0m \u001b[38;5;28mprint\u001b[39m(tr_rep\u001b[38;5;241m.\u001b[39mshape)\n\u001b[0;32m    118\u001b[0m \u001b[38;5;66;03m# Train linear model on embedded samples:\u001b[39;00m\n\u001b[1;32m--> 119\u001b[0m ridge_mod \u001b[38;5;241m=\u001b[39m RidgeClassifierCV(cv \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m4\u001b[39m)\u001b[38;5;241m.\u001b[39mfit(\u001b[43ma\u001b[49m, y_train)\n\u001b[0;32m    120\u001b[0m linear_mod \u001b[38;5;241m=\u001b[39m LogisticRegression(penalty \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m)\u001b[38;5;241m.\u001b[39mfit(a, y_train)\n\u001b[0;32m    122\u001b[0m \u001b[38;5;66;03m# Embed validation samples:\u001b[39;00m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'a' is not defined"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)\n",
    "model = GCN().to(device)\n",
    "#data = train_dataset[0].to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=5e-4)\n",
    "\n",
    "\n",
    "aug = A.RandomChoice([#A.RWSampling(num_seeds=1000, walk_length=10),\n",
    "                      A.NodeDropping(pn=0.1),\n",
    "                      A.FeatureMasking(pf=0.1),\n",
    "                      A.EdgeRemoving(pe=0.1)],\n",
    "                     num_choices=1)\n",
    "\n",
    "val_aug = A.RandomChoice([], num_choices = 0)\n",
    "\n",
    "\n",
    "def barlow(batch):\n",
    "    # Return two random views of input batch\n",
    "    return aug(batch[0], batch[1]), aug(batch[0], batch[1])\n",
    "\n",
    "def off_diagonal(x):\n",
    "    n, m = x.shape\n",
    "    assert n == m\n",
    "    return x.flatten()[:-1].view(n - 1, n + 1)[:, 1:].flatten()\n",
    "\n",
    "class FullGatherLayer(torch.autograd.Function):\n",
    "    \"\"\"\n",
    "    Gather tensors from all process and support backward propagation\n",
    "    for the gradients across processes.\n",
    "    \"\"\"\n",
    "\n",
    "    @staticmethod\n",
    "    def forward(ctx, x):\n",
    "        output = [torch.zeros_like(x) for _ in range(dist.get_world_size())]\n",
    "        dist.all_gather(output, x)\n",
    "        return tuple(output)\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, *grads):\n",
    "        all_gradients = torch.stack(grads)\n",
    "        dist.all_reduce(all_gradients)\n",
    "        return all_gradients[dist.get_rank()]\n",
    "    \n",
    "def VicRegLoss(x, y):\n",
    "    # https://github.com/facebookresearch/vicreg/blob/4e12602fd495af83efd1631fbe82523e6db092e0/main_vicreg.py#L184\n",
    "    # x, y are output of projector(backbone(x and y))\n",
    "    repr_loss = F.mse_loss(x, y)\n",
    "\n",
    "    x = x - x.mean(dim=0)\n",
    "    y = y - y.mean(dim=0)\n",
    "\n",
    "    std_x = torch.sqrt(x.var(dim=0) + 0.0001)\n",
    "    std_y = torch.sqrt(y.var(dim=0) + 0.0001)\n",
    "    std_loss = torch.mean(F.relu(1 - std_x)) / 2 + torch.mean(F.relu(1 - std_y)) / 2\n",
    "\n",
    "    cov_x = (x.T @ x) / (parameters['batch_size'] - 1)\n",
    "    cov_y = (y.T @ y) / (parameters['batch_size'] - 1)\n",
    "    cov_loss = off_diagonal(cov_x).pow_(2).sum().div(\n",
    "        x.shape[1]\n",
    "    ) + off_diagonal(cov_y).pow_(2).sum().div(x.shape[1])\n",
    "    \n",
    "    # self.num_features -> rep_dim?\n",
    "    loss = (\n",
    "        sim_coeff * repr_loss\n",
    "        + std_coeff * std_loss\n",
    "        + cov_coeff * cov_loss\n",
    "    )\n",
    "    return loss\n",
    "\n",
    "sim_coeff = 25\n",
    "std_coeff = 25\n",
    "cov_coeff = 1\n",
    "\n",
    "model.train()\n",
    "for epoch in range(5):\n",
    "    \n",
    "    epo_losses = []\n",
    "    for batch in train_loader:\n",
    "        #batch = batch.to(device)\n",
    "        batch.x = batch.x.float()#.to(device)\n",
    "        #batch.edge_index = batch.edge_index.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Barlow - get 2 random views of batch\n",
    "        b1 = aug(batch.x, batch.edge_index, batch.edge_attr)\n",
    "        b2 = aug(batch.x, batch.edge_index, batch.edge_attr)\n",
    "        \n",
    "                \n",
    "        # Embed each batch (ignoring representations)\n",
    "        [r1, e1], [r2, e2] = model.pair_emb_rep(b1, b2)\n",
    "\n",
    "        # VicReg loss on projections\n",
    "        loss = VicRegLoss(e1, e2)\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        epo_losses.append(loss.data.item())\n",
    "        \n",
    "    print(sum(epo_losses) / len(epo_losses))\n",
    "    \n",
    "    ############################\n",
    "    ## Per-epoch validation step:\n",
    "\n",
    "\n",
    "    # Embed Training Samples:\n",
    "    train_batch = next(iter(train_big_subset))\n",
    "    #print('train batch', train_batch)\n",
    "    train_batch = val_aug(train_batch.x, train_batch.edge_index, train_batch.edge_attr) # val_aug is an empty augmentation\n",
    "    #print('train_batch augd', train_batch)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        tr_rep, _ = model.forward(train_batch)\n",
    "    #print(tr_rep.shape)\n",
    "\n",
    "    # Train linear model on embedded samples:\n",
    "    ridge_mod = RidgeClassifierCV(cv = 4).fit(tr_rep, y_train)\n",
    "    linear_mod = LogisticRegression(penalty = None).fit(tr_rep, y_train)\n",
    "\n",
    "    # Embed validation samples:\n",
    "    val_batch = next(iter(val_loader))\n",
    "    #print('val batch', val_batch)\n",
    "    val_batch = val_aug(val_batch.x, val_batch.edge_index, val_batch.edge_attr) # val_aug is an empty augmentation\n",
    "    #print('val_batch augd', val_batch)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        val_rep, _ = model.forward(val_batch)\n",
    "    #print(val_rep.shape)\n",
    "\n",
    "    # Test linear model on embedded samples:\n",
    "    ridge_score = f1_score(ridge_mod.predict(val_rep), y_val)\n",
    "    linear_score = f1_score(linear_mod.predict(val_rep), y_val)\n",
    "    \n",
    "    print(f'Classifier Scores at Epoch {epoch}:', round(linear_score, 3), round(ridge_score, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e93024f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "if False: # Update for some downstream? Keep in mind this idea of graph masking\n",
    "    # Evaluate\n",
    "    model.eval()\n",
    "    pred = model(data).argmax(dim=1)\n",
    "    correct = (pred[data.test_mask] == data.y[data.test_mask]).sum()\n",
    "    acc = int(correct) / int(data.test_mask.sum())\n",
    "    print(f'Accuracy: {acc:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb308ba1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ca503f6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
