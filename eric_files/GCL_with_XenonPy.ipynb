{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0ca8e8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"printing\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "224beb0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Starting Experiments!\")\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "import torch.distributed as dist\n",
    "\n",
    "import torch_geometric\n",
    "import torch_geometric.nn as gnn\n",
    "\n",
    "from torch_geometric.datasets import QM9\n",
    "import GCL.augmentors\n",
    "import GCL.augmentors as A\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score, mean_squared_error, r2_score\n",
    "from sklearn.linear_model import RidgeClassifierCV, LogisticRegression, LinearRegression\n",
    "import itertools\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "import lightgbm as lgb\n",
    "\n",
    "import GCL.augmentors as A\n",
    "import edge_removing as A_alternate\n",
    "from GCL.augmentors import node_dropping, ppr_diffusion, feature_dropout, edge_adding, rw_sampling\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import math\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "import timeit\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "from rdkit.Chem import PeriodicTable\n",
    "from rdkit import Chem\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "468ade36",
   "metadata": {},
   "outputs": [],
   "source": [
    "#record start time\n",
    "t_0 = timeit.default_timer()\n",
    "# call function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19cbe9f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = {}\n",
    "parameters['batch_size'] = 4096\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85de1a79",
   "metadata": {},
   "outputs": [],
   "source": [
    "#QM9 dataset list of input features and list of target features\n",
    "dataset = \"QM9\"\n",
    "\n",
    "#list of input features in QM9 dataset\n",
    "x_index = {0: 'H atom?',\n",
    "1: 'C atom?',\n",
    "2: 'N atom?',\n",
    "3: 'O atom?',\n",
    "4: 'F atom?',\n",
    "5: 'atomic_number',\n",
    "6: 'aromatic',\n",
    "7: 'sp1',\n",
    "8: 'sp2',\n",
    "9: 'sp3',\n",
    "10: 'num_hs'}\n",
    "x_index_list = ['H atom?', \n",
    "                'C atom?', \n",
    "                'N atom?', \n",
    "                'O atom?', \n",
    "                'F atom?', \n",
    "                'atomic_number', 'aromatic', \n",
    "                'sp1',\n",
    "                'sp2',\n",
    "                'sp3',\n",
    "                'num_hs']\n",
    "\n",
    "\n",
    "#list of target features in QM9 dataset\n",
    "qm9_index = {0: 'Dipole_moment',\n",
    "1: 'Isotropic_polarizability',\n",
    "2: 'HOMO',\n",
    "3: 'LUMO',\n",
    "4: 'HOMO_LUMO_gap',\n",
    "5: 'Electronic_spatial_extent',\n",
    "6: 'Zero_point_vibrational_energy',\n",
    "7: 'Internal_energy_at_0K',\n",
    "8: 'Internal_energy_at_298.15K',\n",
    "9: 'Enthalpy_at_298.15K',\n",
    "10: 'Free_energy_at_298.15K',\n",
    "11: 'Heat_capacity_at_298.15K',\n",
    "12: 'Atomization_energy_at_0K',\n",
    "13: 'Atomization_energy_at_298.15K',\n",
    "14: 'Atomization_enthalpy_at_298.15K',\n",
    "15: 'Atomization_free_energy_at_298.15K',\n",
    "16: 'Rotational_constant_A',\n",
    "17: 'Rotational_constant_B',\n",
    "18: 'Rotational_constant_C'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebe4316f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#parameters for GCL methods\n",
    "\n",
    "tr_batch_size = 1000\n",
    "val_batch_size = 200\n",
    "test_batch_size = 100\n",
    "tr_ratio = 0.9\n",
    "val_ratio = 0.09\n",
    "test_ratio = 0.01\n",
    "num_workers = 2\n",
    "shuffle = True\n",
    "qm9_index_list = ['Dipole_moment', \n",
    "                  'Isotropic_polarizability',\n",
    "                  'Highest_occupied_molecular_orbital_energy',\n",
    "                  'Lowest_unoccupied_molecular_orbital_energy',\n",
    "                  'Gap_between_previous_2',\n",
    "                  'Electronic_spatial_extent',\n",
    "                  'Zero_point_vibrational_energy',\n",
    "                  'Internal_energy_at_0K',\n",
    "                  'Internal_energy_at_298.15K',\n",
    "                  'Enthalpy_at_298.15K',\n",
    "                  'Free_energy_at_298.15K',\n",
    "                  'Heat_capacity_at_298.15K',\n",
    "                  'Atomization_energy_at_0K',\n",
    "                  'Atomization_energy_at_298.15K',\n",
    "                  'Atomization_enthalpy_at_298.15K',\n",
    "                  'Atomization_free_energy_at_298.15K',\n",
    "                  'Rotational_constant_A',\n",
    "                  'Rotational_constant_B',\n",
    "                  'Rotational_constant_C']\n",
    "\n",
    "parameters = {}\n",
    "parameters['tr_batch_size'] = tr_batch_size\n",
    "\n",
    "parameters_used = {}\n",
    "parameters_used['dataset'] = dataset\n",
    "parameters_used['tr_batch_size'] = tr_batch_size\n",
    "parameters_used['val_batch_size'] = val_batch_size\n",
    "parameters_used['test_batch_size'] = test_batch_size\n",
    "parameters_used['tr_ratio'] = tr_ratio\n",
    "parameters_used['val_ratio'] = val_ratio\n",
    "parameters_used['test_ratio'] = test_ratio\n",
    "parameters_used['num_workers'] = num_workers\n",
    "parameters_used['shuffle'] = shuffle\n",
    "parameters_used['target_properties'] = qm9_index_list\n",
    "\n",
    "#vicreg loss function parameters\n",
    "sim_coeff = 25\n",
    "std_coeff = 25\n",
    "cov_coeff = 1\n",
    "\n",
    "#list of training augmentations\n",
    "tr_augmentations = [#A.RWSampling(num_seeds=1000, walk_length=10),\n",
    "                      A.NodeDropping(pn=0.1),\n",
    "                      A.FeatureMasking(pf=0.1),\n",
    "                      A_alternate.EdgeRemoving(pe=0.1)]\n",
    "\n",
    "#list of validation augmentations\n",
    "val_augmentations = []\n",
    "\n",
    "#list of test augmentations\n",
    "test_augmentations = []\n",
    "\n",
    "#number of choices for augmentations for training, validation, and test sets, respectively\n",
    "tr_num_choices = 1\n",
    "val_num_choices = 0\n",
    "test_num_choices = 0\n",
    "\n",
    "#Adam parameters\n",
    "Adam_learning_rate = 0.002\n",
    "Adam_weight_decay = 5e-4\n",
    "adam = {'lr': Adam_learning_rate, \n",
    "        'weight_decay': Adam_weight_decay\n",
    "       }\n",
    "\n",
    "#dictionary of optimizers used\n",
    "optimizers = {}\n",
    "optimizers['adam'] = adam\n",
    "\n",
    "\n",
    "\n",
    "#dictionary of loss functions used\n",
    "loss_functions_used = {}\n",
    "\n",
    "vicreg = {'sim_coeff': sim_coeff,\n",
    "          'std_coeff': std_coeff,\n",
    "          'cov_coeff': cov_coeff\n",
    "         }\n",
    "\n",
    "loss_functions_used['vicreg'] = vicreg\n",
    "\n",
    "augmentations_used = {}\n",
    "augmentations_used['tr_augmentations'] = tr_augmentations\n",
    "augmentations_used['val_augmentations'] = val_augmentations\n",
    "augmentations_used['test_augmentations'] = test_augmentations\n",
    "augmentations_used['tr_num_choices'] = tr_num_choices\n",
    "augmentations_used['val_num_choices'] = val_num_choices\n",
    "augmentations_used['test_num_choices'] = test_num_choices\n",
    "\n",
    "\n",
    "periodic_table = Chem.GetPeriodicTable()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a892b42",
   "metadata": {},
   "outputs": [],
   "source": [
    "#parameters for downstream ML models\n",
    "\n",
    "#dictionary of parameters used for downstream Linear Models\n",
    "lm_parameters = {'default': 'used default for all parameters'}\n",
    "\n",
    "#dictionary of parameters used for downstream Random Forest models\n",
    "rf_parameters = {'n_estimators': 100, \n",
    "                 'max_depth': 10 }\n",
    "\n",
    "#dictionary of parameters used for downstream LightGBM models\n",
    "lgbm_params = {'boosting_type': 'gbdt',\n",
    "               'objective': 'regression',\n",
    "               'metric': {'l2', 'l1'},\n",
    "               'num_leaves': 31,\n",
    "               'learning_rate': 0.05,\n",
    "               'force_col_wise': 'true',\n",
    "               'feature_fraction': 0.9,\n",
    "               'bagging_fraction': 0.8,\n",
    "               'bagging_freq': 5,\n",
    "               'verbose': -1\n",
    "            }\n",
    "lgbm_parameters = {'params': lgbm_params,\n",
    "            'num_boost_round': 20,\n",
    "            'callbacks': [lgb.early_stopping(stopping_rounds=5)]\n",
    "                  }\n",
    "\n",
    "\n",
    "downstream_model_parameters = {}\n",
    "downstream_model_parameters['lm_parameters'] = lm_parameters\n",
    "downstream_model_parameters['rf_parameters'] = rf_parameters\n",
    "downstream_model_parameters['lgbm_parameters'] = lgbm_parameters\n",
    "\n",
    "\n",
    "\n",
    "parameters_used['loss_functions_used'] = loss_functions_used\n",
    "parameters_used['augmentations_used'] = augmentations_used\n",
    "parameters_used['optimizer'] = optimizers\n",
    "parameters_used['adam_learning_rate'] = Adam_learning_rate\n",
    "parameters_used['adam_weight_decay'] = Adam_weight_decay\n",
    "parameters_used['downstream_model_parameters'] = downstream_model_parameters\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a1419ff",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7da3ae94",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9ec9abd",
   "metadata": {},
   "outputs": [],
   "source": [
    "whole_dataset = QM9(root = 'data/')\n",
    "\n",
    "xenonpy_tr_df = pd.read_csv('XenonPy_transformed_datasets/QM9/xenon_tr.csv')\n",
    "xenonpy_val_df = pd.read_csv('XenonPy_transformed_datasets/QM9/xenon_val.csv')\n",
    "xenonpy_test_df = pd.read_csv('XenonPy_transformed_datasets/QM9/xenon_test.csv')\n",
    "\n",
    "\n",
    "\n",
    "n = whole_dataset.len()\n",
    "tr_n = math.floor(tr_ratio*n) # Number of QM9 to use as training data\n",
    "val_n = math.floor(val_ratio*n)\n",
    "test_n = math.floor(test_ratio*n)\n",
    "\n",
    "\n",
    "all_inds = range(n)\n",
    "tr_inds, val_inds = train_test_split(all_inds, train_size = tr_n, random_state = 24)\n",
    "val_test_inds = range(n - tr_n)\n",
    "val_inds, test_inds = train_test_split(val_test_inds, train_size = val_n, random_state = 24)\n",
    "#only run test set at very, very end\n",
    "#keep random state as is, or else rerun XenonPy transformations\n",
    "    #XenonPy files above were created using random_state = 24\n",
    "\n",
    "train_sampler = torch.utils.data.SubsetRandomSampler(tr_inds)\n",
    "val_sampler = torch.utils.data.SubsetRandomSampler(val_inds)\n",
    "test_sampler = torch.utils.data.SubsetRandomSampler(test_inds)\n",
    "\n",
    "# We need to make a train and validation set since QM9 does not provide them\n",
    "train_set = torch.utils.data.Subset(whole_dataset, tr_inds)\n",
    "val_set = torch.utils.data.Subset(whole_dataset, val_inds)\n",
    "test_set = torch.utils.data.Subset(whole_dataset, test_inds)\n",
    "\n",
    "\n",
    "train_loader = torch_geometric.loader.DataLoader(train_set, batch_size = tr_batch_size,\n",
    "                                                shuffle = shuffle, num_workers = num_workers)\n",
    "                                              \n",
    "\n",
    "val_loader = torch_geometric.loader.DataLoader(val_set, batch_size=val_batch_size,\n",
    "                                            shuffle=shuffle, num_workers=num_workers, drop_last=True)\n",
    "                                            \n",
    "test_loader = torch_geometric.loader.DataLoader(test_set, batch_size=test_batch_size,\n",
    "                                            shuffle=shuffle, num_workers=num_workers)\n",
    "                                      \n",
    "    \n",
    "\n",
    "big_train_loader = torch_geometric.loader.DataLoader(train_set, batch_size = int(1e9),\n",
    "                                                shuffle = True, num_workers = 2,) #gets entire training set\n",
    "\n",
    "\n",
    "big_val_loader = torch_geometric.loader.DataLoader(val_set, batch_size = int(1e9),\n",
    "                                                shuffle = True, num_workers = 2,) #gets entire validation set\n",
    "\n",
    "\n",
    "val_aug = A.RandomChoice([], num_choices = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61d45d38",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f5fc0ec",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8efa672c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99c581e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "qm9_index = {0: 'Dipole moment',\n",
    "1: 'Isotropic polarizability',\n",
    "2: 'Highest occupied molecular orbital energy',\n",
    "3: 'Lowest unoccupied molecular orbital energy',\n",
    "4: 'Gap between previous 2',\n",
    "5: 'Electronic spatial extent',\n",
    "6: 'Zero point vibrational energy',\n",
    "7: 'Internal energy at 0K',\n",
    "8: 'Internal energy at 298.15K',\n",
    "9: 'Enthalpy at 298.15K',\n",
    "10: 'Free energy at 298.15K',\n",
    "11: 'Heat capacity at 298.15K',\n",
    "12: 'Atomization energy at 0K',\n",
    "13: 'Atomization energy at 298.15K',\n",
    "14: 'Atomization enthalpy at 298.15K',\n",
    "15: 'Atomization free energy at 298.15K',\n",
    "16: 'Rotational constant A',\n",
    "17: 'Rotational constant B',\n",
    "18: 'Rotational constant C',}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5229ef1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.nn import GCNConv\n",
    "\n",
    "class GCN(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.rep_dim = 128\n",
    "        self.emb_dim = 256\n",
    "        \n",
    "        # Data under graph\n",
    "        self.conv1 = GCNConv(whole_dataset.num_node_features, self.rep_dim // 2)\n",
    "        self.bn1 = nn.BatchNorm1d(self.rep_dim // 2)\n",
    "        self.a1 = nn.LeakyReLU(0.02)\n",
    "        \n",
    "        self.conv2 = GCNConv(self.rep_dim // 2, self.rep_dim) # To Rep Space\n",
    "        self.bn2 = nn.BatchNorm1d(self.rep_dim)\n",
    "        \n",
    "        # Projection to representation\n",
    "        self.mpool1 = gnn.global_mean_pool\n",
    "        #self.fc1 = nn.Linear(self.rep_dim, self.rep_dim)\n",
    "        \n",
    "        # Graph 2\n",
    "        self.conv3 = GCNConv(self.rep_dim, self.rep_dim * 2) # To Emb Space\n",
    "        self.bn3 = nn.BatchNorm1d(self.rep_dim * 2)\n",
    "        \n",
    "        # Projection to embedding\n",
    "        #self.mpool2 = gnn.global_mean_pool\n",
    "        #self.fc2 = nn.Linear(self.emb_dim, self.emb_dim) # Linear to rep?\n",
    "        \n",
    "    def forward(self, data, binds):\n",
    "        x = data[0].float().to(device)\n",
    "        edge_index = data[1].to(device)\n",
    "        \n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = self.a1(self.bn1(x))\n",
    "        x = F.dropout(x, training=self.training)\n",
    "        \n",
    "        x = self.bn2(self.conv2(x, edge_index))\n",
    "        \n",
    "        x_rep = self.mpool1(x, binds)\n",
    "        \n",
    "        x_emb = self.conv3(x, edge_index)\n",
    "        return x_rep, x_emb\n",
    "\n",
    "\n",
    "\n",
    "def off_diagonal(x):\n",
    "    n, m = x.shape\n",
    "    assert n == m\n",
    "    return x.flatten()[:-1].view(n - 1, n + 1)[:, 1:].flatten()\n",
    "\n",
    "def VicRegLoss(x, y):\n",
    "    # https://github.com/facebookresearch/vicreg/blob/4e12602fd495af83efd1631fbe82523e6db092e0/main_vicreg.py#L184\n",
    "    # x, y are output of projector(backbone(x and y))\n",
    "    \n",
    "    # These are the default params used in natural image vicreg\n",
    "    sim_coeff = 25\n",
    "    std_coeff = 25\n",
    "    cov_coeff = 1\n",
    "    \n",
    "    \n",
    "    repr_loss = F.mse_loss(x, y)\n",
    "\n",
    "    x = x - x.mean(dim=0)\n",
    "    y = y - y.mean(dim=0)\n",
    "\n",
    "    std_x = torch.sqrt(x.var(dim=0) + 0.0001)\n",
    "    std_y = torch.sqrt(y.var(dim=0) + 0.0001)\n",
    "    std_loss = torch.mean(F.relu(1 - std_x)) / 2 + torch.mean(F.relu(1 - std_y)) / 2\n",
    "\n",
    "    cov_x = (x.T @ x) / (parameters['batch_size'] - 1)\n",
    "    cov_y = (y.T @ y) / (parameters['batch_size'] - 1)\n",
    "    cov_loss = off_diagonal(cov_x).pow_(2).sum().div(\n",
    "        x.shape[1]\n",
    "    ) + off_diagonal(cov_y).pow_(2).sum().div(x.shape[1])\n",
    "    \n",
    "    # self.num_features -> rep_dim?\n",
    "    loss = (\n",
    "        sim_coeff * repr_loss\n",
    "        + std_coeff * std_loss\n",
    "        + cov_coeff * cov_loss\n",
    "    )\n",
    "    return loss\n",
    "\n",
    "\n",
    "def train(parameters):\n",
    "    \n",
    "    device = 'cuda'\n",
    "\n",
    "    model = GCN().to(device)\n",
    "    n_epochs = 1\n",
    "\n",
    "    \n",
    "    row_ind = 0\n",
    "    \n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=Adam_learning_rate, weight_decay=Adam_weight_decay)\n",
    "    transfer_mat = torch.zeros((len(qm9_index.keys()), 6))\n",
    "\n",
    "    tr_losses = []\n",
    "    val_losses = []\n",
    "\n",
    "    for epoch in range(0,n_epochs+1):\n",
    "        epoch_losses = []\n",
    "        for batch in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            batch_inds = batch.batch.to(device)\n",
    "\n",
    "            # batch of graphs has edge attribs, node attribs - (n_nodes, n_features+1) -> concat (n_nodes, attrib1)\n",
    "\n",
    "            batch.x = batch.x.float()#.to(device)\n",
    "            #batch.edge_index = batch.edge_index.to(device)\n",
    "            \n",
    "            # Barlow - get 2 random views of batch\n",
    "            #print(batch.x, batch.edge_index, batch.edge_attr)\n",
    "            #print(aug, type(aug))\n",
    "            b1 = aug(batch.x, batch.edge_index, batch.edge_attr)\n",
    "            b2 = aug(batch.x, batch.edge_index, batch.edge_attr)\n",
    "            \n",
    "            # Embed each batch (ignoring representations)\n",
    "            r1, e1 = model(b1, batch_inds)\n",
    "            r2, e2 = model(b2, batch_inds)\n",
    "\n",
    "            loss = VicRegLoss(e1, e2)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            epoch_losses.append(loss.data.item())\n",
    "\n",
    "        print('epoch train loss', sum(epoch_losses) / len(epoch_losses))\n",
    "        tr_losses.append(sum(epoch_losses) / len(epoch_losses))\n",
    "\n",
    "        if epoch % 4 == 0:\n",
    "\n",
    "            # Downstream supervised loss\n",
    "            \n",
    "#             for batch in big_train_loader: # take entire train set\n",
    "#                 with torch.no_grad():\n",
    "#                     # Embed training set under model\n",
    "#                     rep_tr, _ = model(val_aug(batch.x, batch.edge_index, batch.edge_attr), batch.batch.to(device))\n",
    "\n",
    "\n",
    "#                     for val_batch in val_loader:\n",
    "#                         # Embed validation set under model\n",
    "#                         rep_val, _ = model(val_aug(val_batch.x, val_batch.edge_index, val_batch.edge_attr), val_batch.batch.to(device))\n",
    "\n",
    "#                         # For each task in QM9\n",
    "#                         for tar_ind in range(batch.y.shape[1]):\n",
    "#                             # Fit a model on model representation of train set\n",
    "\n",
    "#                             #print(rep_tr.shape, batch.y[tar_ind].shap)\n",
    "#                             lm = LinearRegression().fit(rep_tr.cpu(), batch.y[:,tar_ind])\n",
    "#                             # Test the model on model repersentation of val set\n",
    "#                             tar_yhat = lm.predict(rep_val.cpu())\n",
    "#                             mse_met = mean_squared_error(val_batch.y[:,tar_ind], tar_yhat).item()\n",
    "#                             r2_met = r2_score(val_batch.y[:,tar_ind], tar_yhat)\n",
    "#                             #print(qm9_index[tar_ind], mse_met, r2_met)\n",
    "#                             transfer_mat[tar_ind, row_ind] = mse_met\n",
    "#                         row_ind += 1\n",
    "\n",
    "            # VicReg Validation Loss\n",
    "            val_loss = []\n",
    "            for batch in val_loader:\n",
    "                with torch.no_grad():\n",
    "                    # VicReg validation loss\n",
    "                    b1 = aug(batch.x, batch.edge_index, batch.edge_attr)\n",
    "                    b2 = aug(batch.x, batch.edge_index, batch.edge_attr)\n",
    "                    r1, e1 = model(b1, batch.batch.to(device))\n",
    "                    r2, e2 = model(b2, batch.batch.to(device))\n",
    "\n",
    "                    val_loss.append(VicRegLoss(e1, e2).item())\n",
    "\n",
    "            val_losses.append(torch.mean(torch.FloatTensor(val_loss)))\n",
    "\n",
    "    #plt.plot(tr_losses)\n",
    "    plt.plot(val_losses, label = parameters['aug_str'])\n",
    "    \n",
    "    return model, tr_losses, val_losses, transfer_mat\n",
    "\n",
    "import os\n",
    "\n",
    "def trymkdir(path):\n",
    "    if os.path.exists(path):\n",
    "        pass\n",
    "    else:\n",
    "        os.mkdir(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ada17cf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ae5799d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here is sample code for how to implement an \"ablation\" of 2-at-a-time augmentations\n",
    "import GCL.augmentors as A\n",
    "from GCL.augmentors import node_dropping, ppr_diffusion, feature_dropout, edge_adding, rw_sampling\n",
    "\n",
    "aug = A.RandomChoice([#A.RWSampling(num_seeds=1000, walk_length=10),\n",
    "                      A.NodeDropping(pn=0.1),\n",
    "                      A.FeatureMasking(pf=0.1),\n",
    "                      A_alternate.EdgeRemoving(pe=0.1)],\n",
    "                     num_choices=1)\n",
    "\n",
    "print(aug, type(aug))\n",
    "\n",
    "# From a set of augmentations of length n_augmentations\n",
    "aug_set = [A.NodeDropping(pn=0.1), A.FeatureMasking(pf=0.1), A_alternate.EdgeRemoving(pe=0.1), A.EdgeAdding(pe=0.1)]\n",
    "           #ppr_diffusion, feature_dropout, edge_adding, rw_sampling]\n",
    "    #A.PPRDiffusion()\n",
    "aug_strs = ['NodeDropping', 'FeatureMasking', 'EdgeRemoving', 'EdgeAdding']\n",
    "print(aug_strs)\n",
    "\n",
    "# First get all pairs of indexes on-off in a list of length n_augmentations\n",
    "aug_inds = list(itertools.product([0, 1], repeat=len(aug_set)))\n",
    "aug_inds = [x for x in aug_inds if sum(x)==1]\n",
    "print(aug_inds)\n",
    "\n",
    "# Then for each augmentation, train and test a VicReg model trained under that augment\n",
    "parameters = {}\n",
    "parameters['batch_size'] = 64\n",
    "parameters['learning_rate'] = 0.002\n",
    "# etc parameters here which define model, hparams\n",
    "\n",
    "for aug_index in aug_inds:\n",
    "    print(\"aug_index: \", aug_index)\n",
    "    tr_augs = []\n",
    "    tr_strs = []\n",
    "    for ind, augi in enumerate(aug_index):\n",
    "        if augi == 1:\n",
    "            tr_augs.append(aug_set[ind])\n",
    "            tr_strs.append(aug_strs[ind])\n",
    "            \n",
    "    print(\"tr_strs: \", tr_strs[0])\n",
    "\n",
    "    trymkdir(f'aug_sweep3-4/{tr_strs[0]}')\n",
    "    \n",
    "    tr_aug = A.RandomChoice(tr_augs, num_choices = 1)\n",
    "    \n",
    "    #print(tr_aug, type(tr_aug))\n",
    "    parameters['train_aug'] = tr_aug\n",
    "    parameters['aug_str'] = tr_strs\n",
    "    \n",
    "    model, train_loss, val_loss, transfer_mat = train(parameters)\n",
    "    #transfer_scores = transfer_score(model, parameters)\n",
    "    \n",
    "    #print(transfer_mat)\n",
    "    torch.save(model.state_dict(), f'aug_sweep3-4/{tr_strs[0]}/model.pt')\n",
    "    torch.save(train_loss,  f'aug_sweep3-4/{tr_strs[0]}/train_loss.pt')\n",
    "    torch.save(val_loss,  f'aug_sweep3-4/{tr_strs[0]}/val_loss.pt')\n",
    "    torch.save(transfer_mat,  f'aug_sweep3-4/{tr_strs[0]}/transfer_mat.pt')\n",
    "    \n",
    "plt.legend()\n",
    "plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68d245fa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebc3aed6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here is sample code for how to implement an \"ablation\" of 2-at-a-time augmentations\n",
    "\n",
    "\n",
    "# From a set of augmentations of length n_augmentations\n",
    "aug_set = [A.NodeDropping(pn=0.1), A.FeatureMasking(pf=0.1), A_alternate.EdgeRemoving(pe=0.1), A.EdgeAdding(pe=0.1)]\n",
    "           #ppr_diffusion, feature_dropout, edge_adding, rw_sampling]\n",
    "    #A.PPRDiffusion()\n",
    "aug_strs = ['NodeDropping', 'FeatureMasking', 'EdgeRemoving', 'EdgeAdding']\n",
    "print(aug_strs)\n",
    "\n",
    "# First get all pairs of indexes on-off in a list of length n_augmentations\n",
    "aug_inds = list(itertools.product([0, 1], repeat=len(aug_set)))\n",
    "aug_inds = [x for x in aug_inds if sum(x)==3]\n",
    "print(aug_inds)\n",
    "\n",
    "# Then for each augmentation, train and test a VicReg model trained under that augment\n",
    "parameters = {}\n",
    "parameters['batch_size'] = 64\n",
    "parameters['learning_rate'] = 0.002\n",
    "\n",
    "experiment = 'aug_sweep3-4'\n",
    "trymkdir(experiment)\n",
    "# etc parameters here which define model, hparams\n",
    "\n",
    "for aug_index in aug_inds:\n",
    "\n",
    "    tr_augs = []\n",
    "    tr_strs = []\n",
    "    for ind, augi in enumerate(aug_index):\n",
    "        if augi == 1:\n",
    "            tr_augs.append(aug_set[ind])\n",
    "            tr_strs.append(aug_strs[ind])\n",
    "            \n",
    "    print(tr_strs)\n",
    "    trymkdir(f'{experiment}/{tr_strs}')\n",
    "    \n",
    "    tr_aug = A.RandomChoice(tr_augs, num_choices = 1)\n",
    "    \n",
    "    #print(tr_aug, type(tr_aug))\n",
    "    parameters['train_aug'] = tr_aug\n",
    "    parameters['aug_str'] = tr_strs\n",
    "    \n",
    "    model, train_loss, val_loss, transfer_mat = train(parameters)\n",
    "    #transfer_scores = transfer_score(model, parameters)\n",
    "    \n",
    "    #print(transfer_mat)\n",
    "    torch.save(model.state_dict(), f'{experiment}/{tr_strs}/model.pt')\n",
    "    torch.save(train_loss,  f'{experiment}/{tr_strs}/train_loss.pt')\n",
    "    torch.save(val_loss,  f'{experiment}/{tr_strs}/val_loss.pt')\n",
    "    torch.save(transfer_mat,  f'{experiment}/{tr_strs}/transfer_mat.pt')\n",
    "    \n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "600d9df9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7019da0",
   "metadata": {},
   "outputs": [],
   "source": [
    "mse_scores = torch.zeros((19, len(aug_strs)))\n",
    "for i_str, stri in enumerate(aug_strs):\n",
    "    vec = torch.load(f'aug_sweep3-4/{stri}/val_loss.pt')\n",
    "    \n",
    "    model = GCN().to(device)\n",
    "    model.load_state_dict(torch.load(f'aug_sweep3-4/{stri}/model.pt'))\n",
    "    \n",
    "    plt.plot(vec, label = stri)\n",
    "    print(vec)\n",
    "    \n",
    "    for batch in big_train_loader: # take entire train set\n",
    "        with torch.no_grad():\n",
    "            # Embed training set under model\n",
    "            rep_tr, _ = model(val_aug(batch.x, batch.edge_index, batch.edge_attr), batch.batch.to(device))\n",
    "            if torch.cuda.is_available():\n",
    "                rep_tr = rep_tr.to(\"cpu\")\n",
    "            rep_tr = pd.DataFrame(rep_tr.numpy())\n",
    "            rep_tr.join(xenonpy_tr_df)\n",
    "\n",
    "            val_tracker = 0\n",
    "            print(\"One\")\n",
    "            \n",
    "            for val_batch in val_loader:\n",
    "                # Embed validation set under model\n",
    "                rep_val, _ = model(val_aug(val_batch.x, val_batch.edge_index, val_batch.edge_attr), val_batch.batch.to(device))\n",
    "                if torch.cuda.is_available():\n",
    "                    rep_val = rep_val.to(\"cpu\")\n",
    "                rep_val = pd.DataFrame(rep_val.numpy())\n",
    "                rep_val.join(xenonpy_val_df.iloc[val_tracker:(val_tracker+val_batch_size)])\n",
    "                \n",
    "                print(\"val_batch: \", val_batch)\n",
    "                \n",
    "                # For each task in QM9\n",
    "                for tar_ind in range(batch.y.shape[1]):\n",
    "                    # Fit a model on model representation of train set\n",
    "                    print(\"tar_ind: \", tar_ind)\n",
    "                    #print(rep_tr.shape, batch.y[tar_ind].shap)\n",
    "                    lm = LinearRegression().fit(rep_tr, batch.y[:,tar_ind])\n",
    "                    # Test the model on model repersentation of val set\n",
    "                    tar_yhat = lm.predict(rep_val)\n",
    "                    mse_met = mean_squared_error(val_batch.y[:,tar_ind], tar_yhat).item()\n",
    "                    r2_met = r2_score(val_batch.y[:,tar_ind], tar_yhat)\n",
    "                    #print(qm9_index[tar_ind], mse_met, r2_met)\n",
    "                    mse_scores[tar_ind, i_str] = mse_met\n",
    "        \n",
    "                    \n",
    "                val_tracker += val_batch_size\n",
    "        print(\"Five\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce0a4deb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d946cbea",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(mse_scores, mse_scores.shape)\n",
    "\n",
    "# For one score, \n",
    "\n",
    "for i, row in enumerate(mse_scores):\n",
    "    name = qm9_index[i]\n",
    "    \n",
    "    plt.bar(x = range(len(row)), height = row)\n",
    "    plt.xticks(range(len(row)), aug_strs)\n",
    "    plt.xlabel('Single Augmentation')\n",
    "    plt.ylabel('Validation MSE Score')\n",
    "    plt.title(name)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1472474",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a5ae163",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For all-but-one-augmentation\n",
    "mse_scores = torch.zeros((19, len(aug_strs)))\n",
    "rf_mse_scores = torch.zeros((19, len(aug_strs)))\n",
    "lgb_mse_scores = torch.zeros((19, len(aug_strs)))\n",
    "baseline_mse_scores = torch.zeros((19, len(aug_strs)))\n",
    "\n",
    "\n",
    "#augs = os.walk('aug_sweep3-4')\n",
    "print(next(os.walk('aug_sweep3-4'))[1])\n",
    "\n",
    "for i_str, stri in enumerate(next(os.walk('aug_sweep3-4'))[1]):\n",
    "    vec = torch.load(f'aug_sweep3-4/{stri}/val_loss.pt')\n",
    "    \n",
    "    model = GCN().to(device)\n",
    "    model.load_state_dict(torch.load(f'aug_sweep3-4/{stri}/model.pt'))\n",
    "    \n",
    "    plt.plot(vec, label = stri)\n",
    "    print(vec)\n",
    "    \n",
    "    for batch in big_train_loader: # take entire train set\n",
    "        with torch.no_grad():\n",
    "            # Embed training set under model\n",
    "            rep_tr, _ = model(val_aug(batch.x, batch.edge_index, batch.edge_attr), batch.batch.to(device))\n",
    "            if torch.cuda.is_available():\n",
    "                rep_tr = rep_tr.to(\"cpu\")\n",
    "            rep_tr = pd.DataFrame(rep_tr.numpy())\n",
    "            rep_tr.join(xenonpy_tr_df)\n",
    "            \n",
    "    \n",
    "     \n",
    "           \n",
    "            for val_batch in big_val_loader: #take entire val set\n",
    "                # Embed validation set under model\n",
    "                rep_val, _ = model(val_aug(val_batch.x, val_batch.edge_index, val_batch.edge_attr), val_batch.batch.to(device))\n",
    "                if torch.cuda.is_available():\n",
    "                    rep_val = rep_val.to(\"cpu\")\n",
    "                rep_val = pd.DataFrame(rep_val.numpy())\n",
    "                rep_val.join(xenonpy_val_df)\n",
    "              \n",
    "            \n",
    "                # For each task in QM9\n",
    "                for tar_ind in range(batch.y.shape[1]):\n",
    "                    # Fit a model on model representation of train set\n",
    "\n",
    "                    #print(rep_tr.shape, batch.y[tar_ind].shap)\n",
    "                   \n",
    "                    # Test the model on model representation of val set\n",
    "                    lm = LinearRegression().fit(rep_tr, batch.y[:,tar_ind])\n",
    "                    tar_yhat = lm.predict(rep_val)\n",
    "                    mse_met = mean_squared_error(val_batch.y[:,tar_ind], tar_yhat).item()\n",
    "                    r2_met = r2_score(val_batch.y[:,tar_ind], tar_yhat)\n",
    "                    #print(qm9_index[tar_ind], mse_met, r2_met)\n",
    "                    mse_scores[tar_ind, i_str] = mse_met\n",
    "                    print(\"LM MSE for \", qm9_index[tar_ind], \":\", mse_met)\n",
    "                    \n",
    "                    #print(rep_tr.shape, batch.y[tar_ind].shap)\n",
    "                      # Test the model on model representation of val set\n",
    "                    rf = RandomForestRegressor(n_estimators=rf_parameters['n_estimators'], max_depth=rf_parameters['max_depth'], warm_start=True).fit(rep_tr, batch.y[:,tar_ind])\n",
    "                    #rf = rf_list[tar_ind]\n",
    "                    tar_yhat = rf.predict(rep_val)\n",
    "                    mse_met = mean_squared_error(val_batch.y[:,tar_ind], tar_yhat).item()\n",
    "                    rf_mse_scores[tar_ind, i_str] = mse_met\n",
    "                    print(\"RF MSE for \", qm9_index[tar_ind], \":\", mse_met)\n",
    "                    \n",
    "                    lgb_train = lgb.Dataset(rep_tr, batch.y[:,tar_ind])\n",
    "                    lgb_eval = lgb.Dataset(rep_val, val_batch.y[:,tar_ind], reference=lgb_train)\n",
    "                    gbm = lgb.train(lgbm_parameters['params'],\n",
    "                                    lgb_train,\n",
    "                                    num_boost_round=lgbm_parameters['num_boost_round'],\n",
    "                                    valid_sets=lgb_eval,\n",
    "                                    callbacks=lgbm_parameters['callbacks'])\n",
    "                    lgb_yhat = gbm.predict(rep_val, num_iteration=gbm.best_iteration)\n",
    "                    lgb_score = round(mean_squared_error(val_batch.y[:,tar_ind], lgb_yhat), 2)\n",
    "                    lgb_mse_scores[tar_ind, i_str] = lgb_score\n",
    "                    print(\"LGBM MSE for \", qm9_index[tar_ind], \":\", lgb_score)\n",
    "                \n",
    "                    x_val = pd.DataFrame(rep_val.numpy())\n",
    "                    y_tr = pd.DataFrame(batch.y[:,tar_ind]).astype(\"float\")\n",
    "                    means_vector = y_tr.mean(axis = 0)\n",
    "                    rep_means_vectors = means_vector.repeat(x_val.shape[0]) #create a vector where each entry is the mean\n",
    "                    baseline = round(mean_squared_error(val_batch.y[:,tar_ind], rep_means_vectors), 2)\n",
    "                    baseline_mse_scores[tar_ind, i_str] = baseline\n",
    "                    print(\"LGBM MSE for \", qm9_index[tar_ind], \":\", baseline)\n",
    "\n",
    "    \n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bed021ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_dict = {'LM_results':mse_scores, 'RF_results':rf_mse_scores, 'LGB_results':lgb_mse_scores, 'Basic_model':baseline_mse_scores}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7b3d0dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# I would also add a column for \"naive estimator\"\n",
    "# This could be a simple regressor, or a mean estimator (like Eric)\n",
    "\n",
    "for i, row in enumerate(mse_scores):\n",
    "    name = qm9_index[i]\n",
    "    \n",
    "    plt.bar(x = range(len(row)), height = row)\n",
    "    plt.xticks(range(len(row)), ['NOT '+x for x in aug_strs], rotation = -30)\n",
    "    plt.xlabel('Augmentation')\n",
    "    plt.ylabel('Validation MSE Score')\n",
    "    plt.title(name)\n",
    "    \n",
    "    plt.savefig(f'imgs/3-4_{qm9_index[i]}.png', bbox_inches = 'tight')\n",
    "    plt.savefig(f'imgs/3-4_{qm9_index[i]}.pdf', bbox_inches = 'tight')\n",
    "    plt.show()\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a98c6a72",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d208048",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Further step would be summarizing the average ranks of the models to summarize 19 QM's into 1 rank\n",
    "import scipy.stats as ss\n",
    "# For each QM9_index, find the min of the row -> this index is the best augmentation\n",
    "print('PLEASE NOTE THESE ARE RANKS INTEGERS NOT INDEXES:')\n",
    "\n",
    "ranks = []\n",
    "for i, row in enumerate(mse_scores):\n",
    "    elem = torch.argmin(row).item()\n",
    "    \n",
    "    # For each element of the qm9 row, \n",
    "    rank = ss.rankdata(row)\n",
    "    \n",
    "    ranks.append(torch.FloatTensor(rank))\n",
    "    \n",
    "ranks = torch.stack(ranks)\n",
    "print(ranks.shape)\n",
    "meanranks= torch.mean(ranks, dim = 0)\n",
    "print(meanranks)\n",
    "\n",
    "plt.bar(range(4), meanranks)\n",
    "plt.xticks(range(4), ['NOT '+x for x in aug_strs], rotation = -30)\n",
    "plt.ylabel('Average Rank')\n",
    "plt.xlabel('Augmentation')\n",
    "plt.title('Average Augmentation Rank Across Transfer Tasks')\n",
    "plt.savefig(f'imgs/3-4_{qm9_index[i]}.png', bbox_inches = 'tight')\n",
    "plt.savefig(f'imgs/3-4_{qm9_index[i]}.pdf', bbox_inches = 'tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f79a444e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fb221b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# record end time\n",
    "t_1 = timeit.default_timer()\n",
    " \n",
    "# calculate elapsed time\n",
    "elapsed_time = round((t_1 - t_0) , 1)\n",
    "print(f\"Elapsed time: {elapsed_time} seconds\")\n",
    "elapsed_time_minutes = round((elapsed_time/60), 2)\n",
    "print(f\"Elapsed time: {elapsed_time_minutes} minutes\")\n",
    "elapsed_time_hours = round((elapsed_time/3600), 2)\n",
    "print(f\"Elapsed time: {elapsed_time_hours} hours\")\n",
    "\n",
    "other_info = {'dataset':dataset, 'hours':elapsed_time_hours, 'minutes':elapsed_time_minutes, 'seconds':elapsed_time}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "319ed947",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cfe3914",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ede79ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "run = True\n",
    "if run == True:\n",
    "    print(\"Saving results...\")\n",
    "    #save experimental results\n",
    "    current_time = datetime.now()\n",
    "    dt_string = current_time.strftime(\"%Y-%m-%d_%H_%M\")\n",
    "    directory = dt_string\n",
    "    parent_dir = '/home/ewvertina/Molecular_modelling/Experiment_Results/'\n",
    "    path = os.path.join(parent_dir, directory)\n",
    "    os.mkdir(path)\n",
    "    path_state_dict = path + '/state_dict.txt'\n",
    "    path_results_dict = path + '/results_dict.txt'\n",
    "    path_fit_params_dict = path + '/fit_params_dict.txt'\n",
    "    path_runtime = path + '/runtime.txt'\n",
    "    path_parameters = path + '/parameters_used.txt'\n",
    "    path_fig = path + '/train_test_loss.png'\n",
    "    \n",
    "    #save NN model as a torch dictionary\n",
    "    #torch.save(model.state_dict(), path_state_dict)\n",
    "    file = open(path_state_dict, 'w')\n",
    "    file.write(str(model.state_dict()))\n",
    "    file.close()\n",
    "    \n",
    "    #torch.save(results_dict, path_results_dict)\n",
    "    file = open(path_results_dict, 'w')\n",
    "    file.write(str(results_dict))\n",
    "    file.close()\n",
    "    \n",
    "    #torch.save(fit_params_dict, path_fit_params_dict)\n",
    "    file = open(path_fit_params_dict, 'w')\n",
    "    file.write(str(fit_params_dict))\n",
    "    file.close()\n",
    "    \n",
    "    #torch.save(other_info, path_runtime) #save which dataset, runtime\n",
    "    file = open(path_runtime, 'w')\n",
    "    file.write(str(other_info))\n",
    "    file.close()\n",
    "    \n",
    "    #torch.save(parameters_used, path_parameters) #saves all parameters used\n",
    "    file = open(path_parameters, 'w')\n",
    "    file.write(str(parameters_used))\n",
    "    file.close()\n",
    "    \n",
    "    train_test_plot.savefig(path_fig, format='png')\n",
    "    #plt.savefig(path_fig, format='png') #save train-val loss figure\n",
    "    print(\"Saved!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c873757",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c9cea35",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c70c0c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Hello, world!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dcd62cf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4bfdcbd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "943d33cd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e979e70",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45a9ada7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89cff697",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa825928",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
