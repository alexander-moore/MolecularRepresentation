{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "29e6fa5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://arxiv.org/abs/1610.02415\n",
    "\n",
    "# https://pytorch-geometric.readthedocs.io/en/latest/notes/introduction.html\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "import torch.distributed as dist\n",
    "\n",
    "import torch_geometric\n",
    "import torch_geometric.nn as gnn\n",
    "\n",
    "from torch_geometric.datasets import QM9\n",
    "import GCL.augmentors\n",
    "import GCL.augmentors as A\n",
    "import edge_removing as A_alternate\n",
    "from torch_geometric.nn import GCNConv\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score, mean_squared_error\n",
    "from sklearn.linear_model import RidgeClassifierCV, LogisticRegression, LinearRegression\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "import lightgbm as lgb\n",
    "from rdkit.Chem import PeriodicTable\n",
    "from rdkit import Chem\n",
    "from xenonpy.datatools import preset\n",
    "from xenonpy.descriptor import Compositions\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn import preprocessing\n",
    "from matplotlib.pylab import plt\n",
    "from numpy import arange\n",
    "import math\n",
    "import timeit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "be3d6a63",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    " \n",
    "#record start time\n",
    "t_0 = timeit.default_timer()\n",
    "# call function\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3b39d0a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = {}\n",
    "parameters['batch_size'] = 1000\n",
    "periodic_table = Chem.GetPeriodicTable()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fa936b36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of training set:  117747\n",
      "Size of validation set:  11774\n",
      "Size of test set:  1310\n"
     ]
    }
   ],
   "source": [
    "whole_dataset = QM9(root = 'data/')\n",
    "\n",
    "#print(whole_dataset.get_summary())\n",
    "#print(dir(whole_dataset))\n",
    "#print(whole_dataset.len())\n",
    "tr_ratio = 0.9\n",
    "val_ratio = 0.09\n",
    "test_ratio = 0.01\n",
    "\n",
    "\n",
    "n = whole_dataset.len()\n",
    "#print(\"n: \", n)\n",
    "tr_n = math.floor(tr_ratio*n) # Number of QM9 to use as training data\n",
    "val_n = math.floor(val_ratio*n)\n",
    "\n",
    "\n",
    "\n",
    "all_inds = range(n)\n",
    "#print(\"all_inds: \", all_inds)\n",
    "tr_inds, val_inds = train_test_split(all_inds, train_size = tr_n)\n",
    "val_test_inds = range(n - tr_n)\n",
    "#print(\"val_test_inds: \", val_test_inds)\n",
    "val_inds, test_inds = train_test_split(val_test_inds, train_size = val_n)\n",
    "\n",
    "\n",
    "print(\"Size of training set: \", len(tr_inds))\n",
    "print(\"Size of validation set: \", len(val_inds))\n",
    "print(\"Size of test set: \", len(test_inds))\n",
    "#print(type(tr_inds), type(tr_inds[0]))\n",
    "\n",
    "\n",
    "train_sampler = torch.utils.data.SubsetRandomSampler(tr_inds)\n",
    "val_sampler = torch.utils.data.SubsetRandomSampler(val_inds)\n",
    "test_sampler = torch.utils.data.SubsetRandomSampler(test_inds)\n",
    "\n",
    "\n",
    "# We need to make a train and validation set since QM9 does not provide them\n",
    "train_set = torch.utils.data.Subset(whole_dataset, tr_inds)\n",
    "val_set = torch.utils.data.Subset(whole_dataset, val_inds)\n",
    "test_set = torch.utils.data.Subset(whole_dataset, test_inds)\n",
    "\n",
    "train_loader = torch_geometric.loader.DataLoader(train_set, batch_size = parameters['batch_size'],\n",
    "                                                shuffle = True, num_workers = 2,)\n",
    "                                                #sampler = train_sampler)\n",
    "big_train_loader = torch_geometric.loader.DataLoader(train_set, batch_size = int(1e9),\n",
    "                                                shuffle = True, num_workers = 2,)\n",
    "\n",
    "val_loader = torch_geometric.loader.DataLoader(val_set, batch_size=200,\n",
    "                                            shuffle=True, num_workers=2,)\n",
    "                                              #sampler = val_sampler)\n",
    "test_loader = torch_geometric.loader.DataLoader(test_set, batch_size=100,\n",
    "                                            shuffle=True, num_workers=2,)\n",
    "                                              #sampler = val_sampler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "51130aa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_graph_chem_formulae_dictionaries = pd.DataFrame()\n",
    "tr_mol_list = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dcc37f92",
   "metadata": {},
   "outputs": [],
   "source": [
    "qm9_index = {0: 'Dipole moment',\n",
    "1: 'Isotropic polarizability',\n",
    "2: 'Highest occupied molecular orbital energy',\n",
    "3: 'Lowest unoccupied molecular orbital energy',\n",
    "4: 'Gap between previous 2',\n",
    "5: 'Electronic spatial extent',\n",
    "6: 'Zero point vibrational energy',\n",
    "7: 'Internal energy at 0K',\n",
    "8: 'Internal energy at 298.15K',\n",
    "9: 'Enthalpy at 298.15K',\n",
    "10: 'Free energy at 298.15K',\n",
    "11: 'Heat capacity at 298.15K',\n",
    "12: 'Atomization energy at 0K',\n",
    "13: 'Atomization energy at 298.15K',\n",
    "14: 'Atomization enthalpy at 298.15K',\n",
    "15: 'Atomization free energy at 298.15K',\n",
    "16: 'Rotational constant A',\n",
    "17: 'Rotational constant B',\n",
    "18: 'Rotational constant C'}\n",
    "\n",
    "qm9_index_list = ['Dipole moment', \n",
    "                  'Isotropic polarizability',\n",
    "                  'Highest occupied molecular orbital energy',\n",
    "                  'Lowest unoccupied molecular orbital energy',\n",
    "                  'Gap between previous 2',\n",
    "                  'Electronic spatial extent',\n",
    "                  'Zero point vibrational energy',\n",
    "                  'Internal energy at 0K',\n",
    "                  'Internal energy at 298.15K',\n",
    "                  'Enthalpy at 298.15K',\n",
    "                  'Free energy at 298.15K',\n",
    "                  'Heat capacity at 298.15K',\n",
    "                  'Atomization energy at 0K',\n",
    "                  'Atomization energy at 298.15K',\n",
    "                  'Atomization enthalpy at 298.15K',\n",
    "                  'Atomization free energy at 298.15K',\n",
    "                  'Rotational constant A',\n",
    "                  'Rotational constant B',\n",
    "                  'Rotational constant C']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8887ef9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_index = {0: 'H atom?',\n",
    "1: 'C atom?',\n",
    "2: 'N atom?',\n",
    "3: 'O atom?',\n",
    "4: 'F atom?',\n",
    "5: 'atomic_number',\n",
    "6: 'aromatic',\n",
    "7: 'sp1',\n",
    "8: 'sp2',\n",
    "9: 'sp3',\n",
    "10: 'num_hs'}\n",
    "x_index_list = ['H atom?', \n",
    "                'C atom?', \n",
    "                'N atom?', \n",
    "                'O atom?', \n",
    "                'F atom?', \n",
    "                'atomic_number', 'aromatic', \n",
    "                'sp1',\n",
    "                'sp2',\n",
    "                'sp3',\n",
    "                'num_hs']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5fc22a85",
   "metadata": {},
   "outputs": [],
   "source": [
    "def off_diagonal(x):\n",
    "    n, m = x.shape\n",
    "    assert n == m\n",
    "    return x.flatten()[:-1].view(n - 1, n + 1)[:, 1:].flatten()\n",
    "\n",
    "def VicRegLoss(x, y):\n",
    "    # https://github.com/facebookresearch/vicreg/blob/4e12602fd495af83efd1631fbe82523e6db092e0/main_vicreg.py#L184\n",
    "    # x, y are output of projector(backbone(x and y))\n",
    "    repr_loss = F.mse_loss(x, y)\n",
    "\n",
    "    x = x - x.mean(dim=0)\n",
    "    y = y - y.mean(dim=0)\n",
    "\n",
    "    std_x = torch.sqrt(x.var(dim=0) + 0.0001)\n",
    "    std_y = torch.sqrt(y.var(dim=0) + 0.0001)\n",
    "    std_loss = torch.mean(F.relu(1 - std_x)) / 2 + torch.mean(F.relu(1 - std_y)) / 2\n",
    "\n",
    "    cov_x = (x.T @ x) / (parameters['batch_size'] - 1)\n",
    "    cov_y = (y.T @ y) / (parameters['batch_size'] - 1)\n",
    "    cov_loss = off_diagonal(cov_x).pow_(2).sum().div(\n",
    "        x.shape[1]\n",
    "    ) + off_diagonal(cov_y).pow_(2).sum().div(x.shape[1])\n",
    "    \n",
    "    # self.num_features -> rep_dim?\n",
    "    loss = (\n",
    "        sim_coeff * repr_loss\n",
    "        + std_coeff * std_loss\n",
    "        + cov_coeff * cov_loss\n",
    "    )\n",
    "    return loss\n",
    "\n",
    "\n",
    "def atoms_dictionary(atomic_num):\n",
    "    #print(\"atomic_num: \", atomic_num)\n",
    "    atomic_symbol = periodic_table.GetElementSymbol(atomic_num)\n",
    "    return atomic_symbol\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1d49f531",
   "metadata": {},
   "outputs": [],
   "source": [
    "def XenonPy_transform(df, df_dict_column):\n",
    "    cal = Compositions()\n",
    "    comps = df[df_dict_column]\n",
    "    descriptors = cal.transform(comps)\n",
    "    column_names = list(descriptors.columns)\n",
    "    scaler = preprocessing.StandardScaler().fit(descriptors)\n",
    "    descriptors = scaler.transform(descriptors)\n",
    "    descriptors = pd.DataFrame(descriptors, columns = column_names)\n",
    "    return(descriptors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ad42ef50",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mol_dict(batch):\n",
    "    \n",
    "    graph_chem_formulae_dictionaries = pd.DataFrame()\n",
    "    if not graph_chem_formulae_dictionaries.empty:\n",
    "        graph_chem_formulae_dictionaries.drop(columns = 'formula')\n",
    "\n",
    "    node_to_graph_indicator = pd.DataFrame(batch.batch).astype(\"int\")\n",
    "    node = pd.DataFrame(batch.x).astype(\"int\")\n",
    "    mol_list = []\n",
    "    j = 0\n",
    "    mol_dict = {}\n",
    "    for i in range(len(batch.z)):\n",
    "            #get a dictionary for each graph that contains chemical formula\n",
    "                #format for use for XenonPy\n",
    "        if j == int(node_to_graph_indicator.iloc[i]):\n",
    "                #add this ith atom to to the dictionary for the jth graph\n",
    "                #atoms_dictionary(atomic_num)\n",
    "                #call function to add element to molecular dictionary\n",
    "            element = atoms_dictionary(int(node[5].iloc[i]))\n",
    "            if element in mol_dict:\n",
    "                mol_dict[element] = mol_dict[element] + 1\n",
    "            else:\n",
    "                mol_dict[element] = 1\n",
    "        else: #need to move to next graph\n",
    "                #Insert these dictionaries to each row in the df\n",
    "            mol_list.append(mol_dict)\n",
    "            mol_dict = {}\n",
    "            element = atoms_dictionary(int(node[5].iloc[i]))\n",
    "            j += 1\n",
    "\n",
    "    mol_list.append(mol_dict) #need to append the last dict\n",
    "    graph_chem_formulae_dictionaries.insert(0, 'formula', mol_list)\n",
    "    for i in range(len(batch.y) - 1):\n",
    "        if mol_list[i]:\n",
    "            pass\n",
    "        else:\n",
    "            print(\"Empty!!\", \" location: \", i)\n",
    "\n",
    "    return graph_chem_formulae_dictionaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2f1c487",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5307687b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GCN(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.rep_dim = 128\n",
    "        self.emb_dim = 256\n",
    "        \n",
    "        # Data under graph\n",
    "        self.conv1 = GCNConv(whole_dataset.num_node_features, self.rep_dim // 2)\n",
    "        self.bn1 = nn.BatchNorm1d(self.rep_dim // 2)\n",
    "        self.a1 = nn.LeakyReLU(0.02)\n",
    "        \n",
    "        self.conv2 = GCNConv(self.rep_dim // 2, self.rep_dim) # To Rep Space\n",
    "        self.bn2 = nn.BatchNorm1d(self.rep_dim)\n",
    "        \n",
    "        # Projection to representation\n",
    "        self.mpool1 = gnn.global_mean_pool\n",
    "        #self.fc1 = nn.Linear(self.rep_dim, self.rep_dim)\n",
    "        \n",
    "        # Graph 2\n",
    "        self.conv3 = GCNConv(self.rep_dim, self.rep_dim * 2) # To Emb Space\n",
    "        self.bn3 = nn.BatchNorm1d(self.rep_dim * 2)\n",
    "        \n",
    "        # Projection to embedding\n",
    "        self.mpool2 = gnn.global_mean_pool\n",
    "        self.fc2 = nn.Linear(self.emb_dim, self.emb_dim) # Linear to rep?\n",
    "            #might want to get rid of this\n",
    "        \n",
    "    def forward(self, data, binds):\n",
    "        x = data[0].float().to(device)\n",
    "        edge_index = data[1].to(device)\n",
    "        \n",
    "        # Input graph to GConv\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = self.a1(self.bn1(x))\n",
    "        x = F.dropout(x, training=self.training)\n",
    "        \n",
    "        x = self.bn2(self.conv2(x, edge_index))\n",
    "        \n",
    "        # GConv outputs projected to representation space\n",
    "        #print('before pool: ', x.shape)\n",
    "        x_rep = self.mpool1(x, binds)\n",
    "        #print('pooled: ', x_rep.shape)\n",
    "        \n",
    "        #x_rep = self.fc1(x_rep)\n",
    "        #print('projected: ', x_rep.shape, 'gconv', x.shape)\n",
    "        \n",
    "        x_emb = self.bn3(self.conv3(x, edge_index))\n",
    "        #print('x emb after conv3', x_emb.shape)\n",
    "        x_emb = self.mpool2(x_emb, binds)\n",
    "        #print('after pool', x_emb.shape)\n",
    "        x_emb = self.fc2(x_emb)\n",
    "        #print('after fc2', x_emb.shape)\n",
    "        \n",
    "        return x_rep, x_emb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "90bb4b15",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda'\n",
    "\n",
    "model = GCN().to(device)\n",
    "\n",
    "sim_coeff = 25\n",
    "std_coeff = 25\n",
    "cov_coeff = 1\n",
    "\n",
    "aug = A.RandomChoice([#A.RWSampling(num_seeds=1000, walk_length=10),\n",
    "                      A.NodeDropping(pn=0.1),\n",
    "                      A.FeatureMasking(pf=0.1),\n",
    "                      A_alternate.EdgeRemoving(pe=0.1)], #edge_adj was deprecated, so need to use edge_ something instead\n",
    "                      num_choices=1)\n",
    "#should do many other types of augmentations\n",
    "    #train models on all but one augmentations and see which work best\n",
    "        #ablation study!\n",
    "val_aug = A.RandomChoice([], num_choices = 0)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.002, weight_decay=5e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2a4fcd7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#for atom in mol.GetAtoms():\n",
    "                #type_idx.append(types[atom.GetSymbol()])\n",
    "#pseudocode to get molecule's chemical formula/SMILES/etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a76078cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_loss_curves(loss_per_epoch, val_loss):\n",
    "    train_values = loss_per_epoch\n",
    "    val_values = val_loss\n",
    " \n",
    "    # Generate a sequence of integers to represent the epoch numbers\n",
    "    epochs = range(0, len(loss_per_epoch))\n",
    " \n",
    "    # Plot and label the training and validation loss values\n",
    "    plt.plot(epochs, train_values, label='Training Loss')\n",
    "    plt.plot(epochs, val_values, label='Validation Loss')\n",
    " \n",
    "    # Add in a title and axes labels\n",
    "    plt.title('Loss')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    " \n",
    "    # Set the tick locations\n",
    "\n",
    "    plt.xticks(arange(0, len(loss_per_epoch), max(math.floor(len(loss_per_epoch)/10), 1)))\n",
    " \n",
    "    # Display the plot\n",
    "    plt.legend(loc='best')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3d7dff4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start training!\n",
      "epoch 0 train loss: 0.01982753290411613\n",
      "epoch 0 val loss: 0.11311989533631896\n",
      "epoch 1 train loss: 0.02757869398979533\n",
      "epoch 1 val loss: 0.09801480060160737\n",
      "epoch 2 train loss: 0.01993731382792247\n",
      "epoch 2 val loss: 0.09222302194643611\n",
      "epoch 3 train loss: 0.02026521859191725\n",
      "epoch 3 val loss: 0.09294607288184992\n",
      "epoch 4 train loss: 0.019436121434427637\n",
      "epoch 4 val loss: 0.08840640437244579\n",
      "epoch 5 train loss: 0.01983770996579452\n",
      "epoch 5 val loss: 0.0868481967654519\n",
      "epoch 6 train loss: 0.019185237257145733\n",
      "epoch 6 val loss: 0.085824466184989\n",
      "epoch 7 train loss: 0.01914094796325258\n",
      "epoch 7 val loss: 0.08783745818334948\n",
      "epoch 8 train loss: 0.01903340152087768\n",
      "epoch 8 val loss: 0.08481779555341314\n",
      "epoch 9 train loss: 0.019108123301111175\n",
      "epoch 9 val loss: 0.08711187345823246\n",
      "epoch 10 train loss: 0.01901530303445791\n",
      "epoch 10 val loss: 0.08774413069460496\n",
      "epoch 11 train loss: 0.01972957013254667\n",
      "epoch 11 val loss: 0.08858796649904997\n"
     ]
    }
   ],
   "source": [
    "tr_graph_chem_formulae_dictionaries = pd.DataFrame()\n",
    "n_epochs = 100\n",
    "epoch_loss = []\n",
    "val_epoch_loss = []\n",
    "print(\"Start training!\")\n",
    "for epoch in range(0,n_epochs+1):\n",
    "    #print(\"epoch: \", epoch)\n",
    "    epoch_losses = []\n",
    "    for batch in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        batch_inds = batch.batch.to(device)\n",
    "      \n",
    "            \n",
    "        if epoch == 0: #gets dictionary of constituent atoms for each graph for XenonPy\n",
    "            tr_graph_chem_formulae_dictionaries = pd.concat([tr_graph_chem_formulae_dictionaries, get_mol_dict(batch)])     \n",
    "        \n",
    "        # batch of graphs has edge attribs, node attribs - (n_nodes, n_features+1) -> concat (n_nodes, attrib1)\n",
    "\n",
    "        batch.x = batch.x.float()#.to(device)\n",
    "        #batch.edge_index = batch.edge_index.to(device)\n",
    "        #print(\"new_batch: \", batch.z)\n",
    "        # Barlow - get 2 random views of batch\n",
    "        b1 = aug(batch.x, batch.edge_index, batch.edge_attr)\n",
    "        b2 = aug(batch.x, batch.edge_index, batch.edge_attr)\n",
    "\n",
    "        # Embed each batch (ignoring representations)\n",
    "        r1, e1 = model(b1, batch_inds)\n",
    "        r2, e2 = model(b2, batch_inds)\n",
    "\n",
    "        loss = VicRegLoss(e1, e2)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        epoch_losses.append(loss.data.item())\n",
    "        \n",
    "    #epoch_loss.append(sum(epoch_losses) / len(epoch_losses))\n",
    "    #print('epoch', epoch,'train loss:', sum(epoch_losses) / len(epoch_losses))\n",
    "    epoch_loss.append(sum(epoch_losses) / len(tr_inds)) \n",
    "    print('epoch', epoch,'train loss:', sum(epoch_losses) / len(tr_inds))\n",
    "    \n",
    "    \n",
    "    val_loss = 0\n",
    "    model.eval()\n",
    "    \n",
    "    # VicReg Validation Loss\n",
    "    val_epoch_losses = []\n",
    "    for batch in val_loader:\n",
    "        with torch.no_grad():\n",
    "            # VicReg validation loss\n",
    "            b1 = aug(batch.x, batch.edge_index, batch.edge_attr)\n",
    "            b2 = aug(batch.x, batch.edge_index, batch.edge_attr)\n",
    "            r1, e1 = model(b1, batch.batch.to(device))\n",
    "            r2, e2 = model(b2, batch.batch.to(device))\n",
    "                \n",
    "            val_loss = VicRegLoss(e1, e2)\n",
    "            val_epoch_losses.append(val_loss.data.item())\n",
    "            \n",
    "\n",
    "    #val_epoch_loss.append(sum(val_epoch_losses) / len(val_epoch_losses))    \n",
    "    #print('epoch', epoch,'val loss:', sum(val_epoch_losses) / len(val_epoch_losses))\n",
    "    val_epoch_loss.append(sum(val_epoch_losses) / len(val_inds))    \n",
    "    print('epoch', epoch,'val loss:', sum(val_epoch_losses) / len(val_inds))\n",
    "    \n",
    "    \n",
    "    \n",
    "    if epoch == n_epochs:\n",
    "        print(\"Done augmenting!\")\n",
    "        \n",
    "       \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b131812",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(epoch_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd900cd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_loss_curves(epoch_loss, val_epoch_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef83d662",
   "metadata": {},
   "outputs": [],
   "source": [
    "#tr_df_XenonPy = XenonPy_transform(tr_graph_chem_formulae_dictionaries, 'formula')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaaaf3e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(tr_df_XenonPy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5439e6e8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b9bc198",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ca250d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get embedded training set\n",
    "\n",
    "x_tr = pd.DataFrame()\n",
    "x_tr_no_aug = pd.DataFrame()\n",
    "y_tr = pd.DataFrame()\n",
    "y_tr_no_aug = pd.DataFrame()\n",
    "for batch in train_loader: # take entire train set\n",
    "    print(\"batch.x.shape: \", batch.x.shape)\n",
    "    x_tr_tabular_no_aug = pd.DataFrame(batch.x).astype(\"float\")\n",
    "    x_tr_no_aug = pd.concat([x_tr_no_aug, x_tr_tabular_no_aug])\n",
    "    y_tr_tabular_no_aug = pd.DataFrame(batch.y).astype(\"float\")\n",
    "    y_tr_no_aug = pd.concat([y_tr_no_aug, y_tr_tabular_no_aug])\n",
    "    with torch.no_grad():\n",
    "        # Embed training set under model\n",
    "        rep_tr, _ = model(val_aug(batch.x, batch.edge_index, batch.edge_attr), batch.batch.to(device))\n",
    "        if torch.cuda.is_available():\n",
    "            rep_tr = rep_tr.to(\"cpu\")\n",
    "        rep_tr_tabular = pd.DataFrame(rep_tr.numpy())\n",
    "        x_tr = pd.concat([x_tr, rep_tr_tabular])\n",
    "        y_tr_tabular = pd.DataFrame(batch.y).astype(\"float\")\n",
    "        y_tr = pd.concat([y_tr, y_tr_tabular])\n",
    "        \n",
    "        \n",
    "x_val = pd.DataFrame()\n",
    "x_val_no_aug = pd.DataFrame()\n",
    "y_val = pd.DataFrame()\n",
    "y_val_no_aug = pd.DataFrame()\n",
    "val_graph_chem_formulae_dictionaries = pd.DataFrame()\n",
    "for val_batch in val_loader:\n",
    "    print(\"val_batch.x.shape: \", val_batch.x.shape)\n",
    "    x_val_tabular_no_aug = pd.DataFrame(val_batch.x).astype(\"float\")\n",
    "    print(\"x_val_tabular_no_aug: \", x_val_tabular_no_aug)\n",
    "    x_val_no_aug = pd.concat([x_val_no_aug, x_val_tabular_no_aug])\n",
    "    print(\"x_val_no_aug: \", x_val_no_aug)\n",
    "    y_val_tabular_no_aug = pd.DataFrame(val_batch.y).astype(\"float\")\n",
    "    y_val_no_aug = pd.concat([y_val_no_aug, y_val_tabular_no_aug])\n",
    "    val_graph_chem_formulae_dictionaries = pd.concat([val_graph_chem_formulae_dictionaries, get_mol_dict(val_batch)])\n",
    "    with torch.no_grad():\n",
    "        # Embed validation set under model\n",
    "        rep_val, _ = model(val_aug(val_batch.x, val_batch.edge_index, val_batch.edge_attr), val_batch.batch.to(device))\n",
    "        if torch.cuda.is_available():\n",
    "            rep_val = rep_val.to(\"cpu\")\n",
    "        rep_val_tabular = pd.DataFrame(rep_val.numpy())\n",
    "        x_val = pd.concat([x_val, rep_val_tabular])\n",
    "        print(\"x_val: \", x_val)\n",
    "        y_val_tabular = pd.DataFrame(val_batch.y).astype(\"float\")\n",
    "        y_val = pd.concat([y_val, y_val_tabular])\n",
    "        print(\"y_val: \", y_val)\n",
    "\n",
    "x_test = pd.DataFrame()\n",
    "x_test_no_aug = pd.DataFrame()\n",
    "y_test = pd.DataFrame()\n",
    "y_test_no_aug = pd.DataFrame()                \n",
    "for test_batch in test_loader:\n",
    "    x_test_tabular_no_aug = pd.DataFrame(test_batch.x).astype(\"float\")\n",
    "    x_test_no_aug = pd.concat([x_test_no_aug, x_test_tabular_no_aug])\n",
    "    y_test_tabular_no_aug = pd.DataFrame(test_batch.y).astype(\"float\")\n",
    "    y_test_no_aug = pd.concat([y_test_no_aug, y_test_tabular_no_aug])\n",
    "    test_graph_chem_formulae_dictionaries = pd.concat([test_graph_chem_formulae_dictionaries, get_mol_dict(test_batch)])\n",
    "    with torch.no_grad():\n",
    "        # Embed validation set under model\n",
    "        rep_test, _ = model(test_aug(test_batch.x, test_batch.edge_index, test_batch.edge_attr), test_batch.batch.to(device))\n",
    "        if torch.cuda.is_available():\n",
    "            rep_test = rep_test.to(\"cpu\")\n",
    "        rep_test_tabular = pd.DataFrame(rep_test.numpy())\n",
    "        x_test = pd.concat([x_test, rep_test_tabular])\n",
    "        y_test_tabular = pd.DataFrame(test_batch.y).astype(\"float\")\n",
    "        y_test = pd.concat([y_test, y_test_tabular])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6ebe83f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(\"x_val_no_aug: \", x_val_no_aug)\n",
    "print(\"y_val_no_aug: \", y_val_no_aug.shape)\n",
    "print(\"x_val: \", x_val)\n",
    "print(\"y_val: \", y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c8ac524",
   "metadata": {},
   "outputs": [],
   "source": [
    "#val_df_XenonPy = XenonPy_transform(val_graph_chem_formulae_dictionaries, 'formula')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abe17475",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(x_val_no_aug)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32df2b88",
   "metadata": {},
   "outputs": [],
   "source": [
    "#val_df_XenonPy.rename(lambda x: str(x), axis='columns')\n",
    "#val_df_XenonPy.columns = val_df_XenonPy.columns.astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c35a22a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#for i in val_df_XenonPy:\n",
    "    #print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6808e879",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3177b64b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#a function to run linear models\n",
    "def linear_models(x_train, x_test, y_train, y_test, list_target_features):\n",
    "    # For each task in QM9\n",
    "    means_vector = y_train.mean(axis = 0)\n",
    "    rep_means_vectors = means_vector.repeat(x_train.shape[0]) #create a vector where each entry is the mean\n",
    "    for target_feature in range(y_test.shape[1]):\n",
    "\n",
    "        # Fit a model on model representation of train set:\n",
    "        #lm = LinearRegression().fit(x_train.values, y_train[target_feature].values)\n",
    "        \n",
    "        #Fit Random Forest models here:\n",
    "        #rf = RandomForestRegressor(n_estimators=10, max_depth=10 )\n",
    "        #rf.fit(x_train, y_train[target_feature])\n",
    "        #rf_yhat = rf.predict(x_test)\n",
    "        \n",
    "        #Fit LightGBM models here (LightGBM is supposedly better than XGBoost):\n",
    "        lgb_train = lgb.Dataset(x_train.values, y_train[target_feature].values, params={'verbose': -1})\n",
    "        lgb_eval = lgb.Dataset(x_test.values, y_test[target_feature].values, reference=lgb_train, params={'verbose': -1})\n",
    "        params = {\n",
    "            'boosting_type': 'gbdt',\n",
    "            'objective': 'regression',\n",
    "            'metric': {'l2', 'l1'},\n",
    "            'num_leaves': 31,\n",
    "            'learning_rate': 0.05,\n",
    "            'force_col_wise': 'true',\n",
    "            'feature_fraction': 0.9,\n",
    "            'bagging_fraction': 0.8,\n",
    "            'bagging_freq': 5,\n",
    "            'verbose': -1\n",
    "        }\n",
    "        \n",
    "        gbm = lgb.train(params,\n",
    "                        lgb_train,\n",
    "                        num_boost_round=20,\n",
    "                        valid_sets=lgb_eval,\n",
    "                        callbacks=[lgb.early_stopping(stopping_rounds=5)])\n",
    "        lgb_yhat = gbm.predict(x_test.values, num_iteration=gbm.best_iteration)\n",
    "        \n",
    "        \n",
    "        # Test the model on model repersentation of val set\n",
    "        #yhat = lm.predict(x_test.values)\n",
    "        #score = mean_squared_error(y_test[target_feature].values, yhat)\n",
    "        #rf_score = mean_squared_error(y_test[target_feature], rf_yhat)\n",
    "        lgb_score = mean_squared_error(y_test[target_feature].values, lgb_yhat)\n",
    "        rep_means_vectors = means_vector[target_feature].repeat(x_test.shape[0])\n",
    "        baseline = mean_squared_error(y_test[target_feature].values, rep_means_vectors)\n",
    "        #baseline is a model that always outputs the mean of the training sample\n",
    "        print(\"Baseline MSE for \", list_target_features[target_feature], \": \", baseline)\n",
    "        #print(\"Linear Regression Model MSE for \", list_target_features[target_feature], \": \", score)\n",
    "        #print(\"RF Model Mean-Squared-Error for \", list_target_features[target_feature], \": \", rf_score)\n",
    "        print(\"LightGBM Model MSE for \", list_target_features[target_feature], \": \", lgb_score)\n",
    "        \n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cde40c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#need to make sure that I am getting the correct graph for y's"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6a33d02",
   "metadata": {},
   "outputs": [],
   "source": [
    "#concatenate XenonPy transformations with x_tr and x_val\n",
    "#x_tr = pd.concat([x_tr, tr_df_XenonPy])\n",
    "#x_val = pd.concat([x_val, val_df_XenonPy])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99d75e5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_tr = pd.concat([x_tr, x_val]) #tr and val combined for training set\n",
    "y_tr = pd.concat([x_tr, x_val]) #tr and val combined for training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbbf5b47",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "linear_models(x_tr, x_test, y_tr, y_test, qm9_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4911fc9f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e0d95f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#linear_models(x_tr_no_aug, x_val_no_aug, y_tr_no_aug, y_val_no_aug, qm9_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a3be90c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abbfbbd4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91e98619",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71a03b02",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Hello, world!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a9fdd28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# record end time\n",
    "t_1 = timeit.default_timer()\n",
    " \n",
    "# calculate elapsed time and print\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdc76a88",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96262e32",
   "metadata": {},
   "outputs": [],
   "source": [
    "elapsed_time = round((t_1 - t_0) , 1)\n",
    "print(f\"Elapsed time: {elapsed_time} seconds\")\n",
    "elapsed_time_minutes = round((elapsed_time/60), 2)\n",
    "print(f\"Elapsed time: {elapsed_time_minutes} minutes\")\n",
    "elapsed_time_hours = round((elapsed_time/3600), 2)\n",
    "print(f\"Elapsed time: {elapsed_time_hours} hours\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d020e298",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb70c3f3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "188f0640",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48c6280b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66527be7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f65cb39f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "304940bb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23057d75",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdeae51b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4811a425",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2717af52",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35fe50c1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4885e41b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eebdb5db",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6c40dce",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4bb4d1f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4edfaa43",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b67d5ce",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a9f9fef",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddfc35b3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6a46903",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
