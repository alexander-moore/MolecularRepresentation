{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "29e6fa5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Code is running!\n"
     ]
    }
   ],
   "source": [
    "#Code to create a baseline set of models - ones without augmentation\n",
    "#This code transforms the input data x, which is data in the node-space, to the size of the representation space\n",
    "#Need to do this to create a true baseline\n",
    "\n",
    "# https://arxiv.org/abs/1610.02415\n",
    "\n",
    "# https://pytorch-geometric.readthedocs.io/en/latest/notes/introduction.html\n",
    "print(\"Code is running!\")\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "import torch.distributed as dist\n",
    "\n",
    "import torch_geometric\n",
    "import torch_geometric.nn as gnn\n",
    "\n",
    "from torch_geometric.datasets import QM9\n",
    "import GCL.augmentors\n",
    "import GCL.augmentors as A\n",
    "import edge_removing as A_alternate\n",
    "from torch_geometric.nn import GCNConv\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score, mean_squared_error, r2_score\n",
    "from sklearn.linear_model import RidgeClassifierCV, LogisticRegression, LinearRegression\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "import lightgbm as lgb\n",
    "from rdkit.Chem import PeriodicTable\n",
    "from rdkit import Chem\n",
    "from xenonpy.datatools import preset\n",
    "from xenonpy.descriptor import Compositions\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn import preprocessing\n",
    "from matplotlib.pylab import plt\n",
    "from numpy import arange\n",
    "import math\n",
    "import timeit\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "import ipynb.fs.full.XenonPy_transformation as XPy\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e72ca970",
   "metadata": {},
   "outputs": [],
   "source": [
    "#the following are needed to use PyTorch Lightning\n",
    "from functools import partial\n",
    "from typing import Sequence, Tuple, Union\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pytorch_lightning as pl\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.transforms.functional as VisionF\n",
    "from pytorch_lightning import Callback, LightningModule, Trainer\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "from torch import Tensor\n",
    "from torch.utils.data import DataLoader\n",
    "from torchmetrics.functional import accuracy\n",
    "from torchvision.datasets import CIFAR10\n",
    "from torchvision.models.resnet import resnet18\n",
    "from torchvision.utils import make_grid\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "be3d6a63",
   "metadata": {},
   "outputs": [],
   "source": [
    "#record start time\n",
    "t_0 = timeit.default_timer()\n",
    "# call function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "984ef87e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#QM9 dataset list of input features and list of target features\n",
    "dataset = \"QM9\"\n",
    "\n",
    "#list of input features in QM9 dataset\n",
    "x_index = {0: 'H atom?',\n",
    "1: 'C atom?',\n",
    "2: 'N atom?',\n",
    "3: 'O atom?',\n",
    "4: 'F atom?',\n",
    "5: 'atomic_number',\n",
    "6: 'aromatic',\n",
    "7: 'sp1',\n",
    "8: 'sp2',\n",
    "9: 'sp3',\n",
    "10: 'num_hs'}\n",
    "x_index_list = ['H atom?', \n",
    "                'C atom?', \n",
    "                'N atom?', \n",
    "                'O atom?', \n",
    "                'F atom?', \n",
    "                'atomic_number', 'aromatic', \n",
    "                'sp1',\n",
    "                'sp2',\n",
    "                'sp3',\n",
    "                'num_hs']\n",
    "\n",
    "\n",
    "#list of target features in QM9 dataset\n",
    "qm9_index = {0: 'Dipole_moment',\n",
    "1: 'Isotropic_polarizability',\n",
    "2: 'HOMO',\n",
    "3: 'LUMO',\n",
    "4: 'HOMO_LUMO_gap',\n",
    "5: 'Electronic_spatial_extent',\n",
    "6: 'Zero_point_vibrational_energy',\n",
    "7: 'Internal_energy_at_0K',\n",
    "8: 'Internal_energy_at_298.15K',\n",
    "9: 'Enthalpy_at_298.15K',\n",
    "10: 'Free_energy_at_298.15K',\n",
    "11: 'Heat_capacity_at_298.15K',\n",
    "12: 'Atomization_energy_at_0K',\n",
    "13: 'Atomization_energy_at_298.15K',\n",
    "14: 'Atomization_enthalpy_at_298.15K',\n",
    "15: 'Atomization_free_energy_at_298.15K',\n",
    "16: 'Rotational_constant_A',\n",
    "17: 'Rotational_constant_B',\n",
    "18: 'Rotational_constant_C'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3b39d0a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#parameters for GCL methods\n",
    "\n",
    "tr_batch_size = 1000\n",
    "val_batch_size = 200\n",
    "test_batch_size = 100\n",
    "tr_ratio = 0.9\n",
    "val_ratio = 0.09\n",
    "test_ratio = 0.01\n",
    "num_workers = 2\n",
    "shuffle = True\n",
    "qm9_index_list = ['Dipole_moment', \n",
    "                  'Isotropic_polarizability',\n",
    "                  'Highest_occupied_molecular_orbital_energy',\n",
    "                  'Lowest_unoccupied_molecular_orbital_energy',\n",
    "                  'Gap_between_previous_2',\n",
    "                  'Electronic_spatial_extent',\n",
    "                  'Zero_point_vibrational_energy',\n",
    "                  'Internal_energy_at_0K',\n",
    "                  'Internal_energy_at_298.15K',\n",
    "                  'Enthalpy_at_298.15K',\n",
    "                  'Free_energy_at_298.15K',\n",
    "                  'Heat_capacity_at_298.15K',\n",
    "                  'Atomization_energy_at_0K',\n",
    "                  'Atomization_energy_at_298.15K',\n",
    "                  'Atomization_enthalpy_at_298.15K',\n",
    "                  'Atomization_free_energy_at_298.15K',\n",
    "                  'Rotational_constant_A',\n",
    "                  'Rotational_constant_B',\n",
    "                  'Rotational_constant_C']\n",
    "\n",
    "parameters = {}\n",
    "parameters['tr_batch_size'] = tr_batch_size\n",
    "\n",
    "parameters_used = {}\n",
    "parameters_used['dataset'] = dataset\n",
    "parameters_used['tr_batch_size'] = tr_batch_size\n",
    "parameters_used['val_batch_size'] = val_batch_size\n",
    "parameters_used['test_batch_size'] = test_batch_size\n",
    "parameters_used['tr_ratio'] = tr_ratio\n",
    "parameters_used['val_ratio'] = val_ratio\n",
    "parameters_used['test_ratio'] = test_ratio\n",
    "parameters_used['num_workers'] = num_workers\n",
    "parameters_used['shuffle'] = shuffle\n",
    "parameters_used['target_properties'] = qm9_index_list\n",
    "\n",
    "#vicreg loss function parameters\n",
    "sim_coeff = 25\n",
    "std_coeff = 25\n",
    "cov_coeff = 1\n",
    "\n",
    "#list of training augmentations\n",
    "#leaving this empty can get the baseline???\n",
    "tr_augmentations = [] \n",
    "\n",
    "#list of validation augmentations\n",
    "val_augmentations = []\n",
    "\n",
    "#list of test augmentations\n",
    "test_augmentations = []\n",
    "\n",
    "#number of choices for augmentations for training, validation, and test sets, respectively\n",
    "tr_num_choices = 1\n",
    "val_num_choices = 0\n",
    "test_num_choices = 0\n",
    "\n",
    "#Adam parameters\n",
    "Adam_learning_rate = 0.002\n",
    "Adam_weight_decay = 5e-4\n",
    "adam = {'lr': Adam_learning_rate, \n",
    "        'weight_decay': Adam_weight_decay\n",
    "       }\n",
    "\n",
    "#dictionary of optimizers used\n",
    "optimizers = {}\n",
    "optimizers['adam'] = adam\n",
    "\n",
    "\n",
    "\n",
    "#dictionary of loss functions used\n",
    "loss_functions_used = {}\n",
    "\n",
    "vicreg = {'sim_coeff': sim_coeff,\n",
    "          'std_coeff': std_coeff,\n",
    "          'cov_coeff': cov_coeff\n",
    "         }\n",
    "\n",
    "loss_functions_used['vicreg'] = vicreg\n",
    "\n",
    "augmentations_used = {}\n",
    "augmentations_used['tr_augmentations'] = tr_augmentations\n",
    "augmentations_used['val_augmentations'] = val_augmentations\n",
    "augmentations_used['test_augmentations'] = test_augmentations\n",
    "augmentations_used['tr_num_choices'] = tr_num_choices\n",
    "augmentations_used['val_num_choices'] = val_num_choices\n",
    "augmentations_used['test_num_choices'] = test_num_choices\n",
    "\n",
    "\n",
    "periodic_table = Chem.GetPeriodicTable()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ee7d479b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#parameters for downstream ML models\n",
    "\n",
    "#dictionary of parameters used for downstream Linear Models\n",
    "lm_parameters = {'default': 'used default for all parameters'}\n",
    "\n",
    "#dictionary of parameters used for downstream Random Forest models\n",
    "rf_parameters = {'n_estimators': 2, \n",
    "                 'max_depth': 10 }\n",
    "\n",
    "#dictionary of parameters used for downstream LightGBM models\n",
    "lgbm_params = {'boosting_type': 'gbdt',\n",
    "               'objective': 'regression',\n",
    "               'metric': {'l2', 'l1'},\n",
    "               'num_leaves': 31,\n",
    "               'learning_rate': 0.05,\n",
    "               'force_col_wise': 'true',\n",
    "               'feature_fraction': 0.9,\n",
    "               'bagging_fraction': 0.8,\n",
    "               'bagging_freq': 5,\n",
    "               'verbose': -1\n",
    "            }\n",
    "lgbm_parameters = {'params': lgbm_params,\n",
    "            'num_boost_round': 20,\n",
    "            'callbacks': [lgb.early_stopping(stopping_rounds=5)]\n",
    "                  }\n",
    "\n",
    "\n",
    "downstream_model_parameters = {}\n",
    "downstream_model_parameters['lm_parameters'] = lm_parameters\n",
    "downstream_model_parameters['rf_parameters'] = rf_parameters\n",
    "downstream_model_parameters['lgbm_parameters'] = lgbm_parameters\n",
    "\n",
    "\n",
    "\n",
    "parameters_used['loss_functions_used'] = loss_functions_used\n",
    "parameters_used['augmentations_used'] = augmentations_used\n",
    "parameters_used['optimizer'] = optimizers\n",
    "parameters_used['adam_learning_rate'] = Adam_learning_rate\n",
    "parameters_used['adam_weight_decay'] = Adam_weight_decay\n",
    "parameters_used['downstream_model_parameters'] = downstream_model_parameters\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fa936b36",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load the dataset through tr, val, and test data loaders\n",
    "\n",
    "whole_dataset = QM9(root = 'data/')\n",
    "\n",
    "n = whole_dataset.len()\n",
    "tr_n = math.floor(tr_ratio*n) # Number of QM9 to use as training data\n",
    "val_n = math.floor(val_ratio*n)\n",
    "test_n = math.floor(test_ratio*n)\n",
    "\n",
    "\n",
    "all_inds = range(n)\n",
    "tr_inds, val_inds = train_test_split(all_inds, train_size = tr_n, random_state = 24)\n",
    "val_test_inds = range(n - tr_n)\n",
    "val_inds, test_inds = train_test_split(val_test_inds, train_size = val_n, random_state = 24)\n",
    "\n",
    "\n",
    "print(\"Size of training set: \", len(tr_inds))\n",
    "print(\"Size of validation set: \", len(val_inds))\n",
    "print(\"Size of test set: \", len(test_inds))\n",
    "\n",
    "\n",
    "train_sampler = torch.utils.data.SubsetRandomSampler(tr_inds)\n",
    "val_sampler = torch.utils.data.SubsetRandomSampler(val_inds)\n",
    "test_sampler = torch.utils.data.SubsetRandomSampler(test_inds)\n",
    "\n",
    "\n",
    "# We need to make a train and validation set since QM9 does not provide them\n",
    "train_set = torch.utils.data.Subset(whole_dataset, tr_inds)\n",
    "val_set = torch.utils.data.Subset(whole_dataset, val_inds)\n",
    "test_set = torch.utils.data.Subset(whole_dataset, test_inds)\n",
    "\n",
    "train_loader = torch_geometric.loader.DataLoader(train_set, batch_size = tr_batch_size,\n",
    "                                                shuffle = shuffle, num_workers = num_workers)\n",
    "                                                #sampler = train_sampler)\n",
    "\n",
    "val_loader = torch_geometric.loader.DataLoader(val_set, batch_size=val_batch_size,\n",
    "                                            shuffle=shuffle, num_workers=num_workers, drop_last=True)\n",
    "                                              #sampler = val_sampler)\n",
    "test_loader = torch_geometric.loader.DataLoader(test_set, batch_size=test_batch_size,\n",
    "                                            shuffle=shuffle, num_workers=num_workers)\n",
    "                                              #sampler = val_sampler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9e6dc7b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ca46dfc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5fc22a85",
   "metadata": {},
   "outputs": [],
   "source": [
    "def off_diagonal(x):\n",
    "    n, m = x.shape\n",
    "    assert n == m\n",
    "    return x.flatten()[:-1].view(n - 1, n + 1)[:, 1:].flatten()\n",
    "\n",
    "def VicRegLoss(x, y):\n",
    "    # https://github.com/facebookresearch/vicreg/blob/4e12602fd495af83efd1631fbe82523e6db092e0/main_vicreg.py#L184\n",
    "    # x, y are output of projector(backbone(x and y))\n",
    "    repr_loss = F.mse_loss(x, y)\n",
    "\n",
    "    x = x - x.mean(dim=0)\n",
    "    y = y - y.mean(dim=0)\n",
    "\n",
    "    std_x = torch.sqrt(x.var(dim=0) + 0.0001)\n",
    "    std_y = torch.sqrt(y.var(dim=0) + 0.0001)\n",
    "    std_loss = torch.mean(F.relu(1 - std_x)) / 2 + torch.mean(F.relu(1 - std_y)) / 2\n",
    "\n",
    "    cov_x = (x.T @ x) / (tr_batch_size - 1)\n",
    "    cov_y = (y.T @ y) / (tr_batch_size - 1)\n",
    "    cov_loss = off_diagonal(cov_x).pow_(2).sum().div(\n",
    "        x.shape[1]\n",
    "    ) + off_diagonal(cov_y).pow_(2).sum().div(x.shape[1])\n",
    "    \n",
    "    # self.num_features -> rep_dim?\n",
    "    loss = (\n",
    "        sim_coeff * repr_loss\n",
    "        + std_coeff * std_loss\n",
    "        + cov_coeff * cov_loss\n",
    "    )\n",
    "    return loss\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d49f531",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2f1c487",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5307687b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GCN(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.rep_dim = 128\n",
    "        self.emb_dim = 256\n",
    "        \n",
    "        # Data under graph\n",
    "        self.conv1 = GCNConv(whole_dataset.num_node_features, self.rep_dim // 2)\n",
    "        self.bn1 = nn.BatchNorm1d(self.rep_dim // 2)\n",
    "        self.a1 = nn.LeakyReLU(0.02)\n",
    "        \n",
    "        self.conv2 = GCNConv(self.rep_dim // 2, self.rep_dim) # To Rep Space\n",
    "        self.bn2 = nn.BatchNorm1d(self.rep_dim)\n",
    "        \n",
    "        # Projection to representation\n",
    "        self.mpool1 = gnn.global_mean_pool\n",
    "        #self.fc1 = nn.Linear(self.rep_dim, self.rep_dim)\n",
    "        \n",
    "        # Graph 2\n",
    "        self.conv3 = GCNConv(self.rep_dim, self.rep_dim * 2) # To Emb Space\n",
    "        self.bn3 = nn.BatchNorm1d(self.rep_dim * 2)\n",
    "        \n",
    "        # Projection to embedding\n",
    "        self.mpool2 = gnn.global_mean_pool\n",
    "        self.fc2 = nn.Linear(self.emb_dim, self.emb_dim) # Linear to rep?\n",
    "            #might want to get rid of this\n",
    "        \n",
    "    def forward(self, data, binds):\n",
    "        x = data[0].float().to(device)\n",
    "        edge_index = data[1].to(device)\n",
    "        \n",
    "        # Input graph to GConv\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = self.a1(self.bn1(x))\n",
    "        x = F.dropout(x, training=self.training)\n",
    "        \n",
    "        x = self.bn2(self.conv2(x, edge_index))\n",
    "        \n",
    "        # GConv outputs projected to representation space\n",
    "        #print('before pool: ', x.shape)\n",
    "        x_rep = self.mpool1(x, binds)\n",
    "        #print('pooled: ', x_rep.shape)\n",
    "        \n",
    "        #x_rep = self.fc1(x_rep)\n",
    "        #print('projected: ', x_rep.shape, 'gconv', x.shape)\n",
    "        \n",
    "        x_emb = self.bn3(self.conv3(x, edge_index))\n",
    "        #print('x emb after conv3', x_emb.shape)\n",
    "        x_emb = self.mpool2(x_emb, binds)\n",
    "        #print('after pool', x_emb.shape)\n",
    "        x_emb = self.fc2(x_emb)\n",
    "        #print('after fc2', x_emb.shape)\n",
    "        \n",
    "        return x_rep, x_emb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90bb4b15",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99fc5cbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "99fec89a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def atoms_dictionary(atomic_num):\n",
    "    atomic_symbol = periodic_table.GetElementSymbol(atomic_num)\n",
    "    return atomic_symbol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2a4fcd7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mol_dict(batch): #Give a chemical formula\n",
    "\n",
    "        graph_chem_formulae_dictionaries = pd.DataFrame()\n",
    "        if not graph_chem_formulae_dictionaries.empty:\n",
    "            graph_chem_formulae_dictionaries.drop(columns = 'formula')\n",
    "\n",
    "        node_to_graph_indicator = pd.DataFrame(batch.batch).astype(\"int\")\n",
    "        node = pd.DataFrame(batch.x).astype(\"int\")\n",
    "        mol_list = []\n",
    "        j = 0\n",
    "        mol_dict = {}\n",
    "        for i in range(len(batch.z)):\n",
    "                #get a dictionary for each graph that contains chemical formula\n",
    "                    #format for use for XenonPy\n",
    "            if j == int(node_to_graph_indicator.iloc[i]):\n",
    "                    #add this ith atom to to the dictionary for the jth graph\n",
    "                    #atoms_dictionary(atomic_num)\n",
    "                    #call function to add element to molecular dictionary\n",
    "                element = atoms_dictionary(int(node[5].iloc[i]))\n",
    "                if element in mol_dict:\n",
    "                    mol_dict[element] = mol_dict[element] + 1\n",
    "                else:\n",
    "                    mol_dict[element] = 1\n",
    "            else: #need to move to next graph\n",
    "                    #Insert these dictionaries to each row in the df\n",
    "                mol_list.append(mol_dict)\n",
    "                mol_dict = {}\n",
    "                element = atoms_dictionary(int(node[5].iloc[i]))\n",
    "                j += 1\n",
    "\n",
    "        mol_list.append(mol_dict) #need to append the last dict\n",
    "        graph_chem_formulae_dictionaries.insert(0, 'formula', mol_list)\n",
    "        for i in range(len(batch.y) - 1):\n",
    "            if mol_list[i]:\n",
    "                pass\n",
    "            else:\n",
    "                print(\"Empty!!\", \" location: \", i)\n",
    "\n",
    "        return graph_chem_formulae_dictionaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a76078cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_loss_curves(loss_per_epoch, val_loss):\n",
    "    train_values = loss_per_epoch\n",
    "    val_values = val_loss\n",
    " \n",
    "    # Generate a sequence of integers to represent the epoch numbers\n",
    "    epochs = range(0, len(loss_per_epoch))\n",
    " \n",
    "    # Plot and label the training and validation loss values\n",
    "    plt.plot(epochs, train_values, label='Training Loss')\n",
    "    plt.plot(epochs, val_values, label='Validation Loss')\n",
    " \n",
    "    # Add in a title and axes labels\n",
    "    plt.title('Loss')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    " \n",
    "    # Set the tick locations\n",
    "\n",
    "    plt.xticks(arange(0, len(loss_per_epoch), max(math.floor(len(loss_per_epoch)/10), 1)))\n",
    " \n",
    "    # Display the plot\n",
    "    plt.legend(loc='best')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f45c70f1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50798953",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "62495a21",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda'\n",
    "model = GCN().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a0e354e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#augmentations and optimizers initializations\n",
    "\n",
    "#should do many other types of augmentations\n",
    "    #train models on all but one augmentations and see which work best\n",
    "        #ablation study!\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=Adam_learning_rate, weight_decay=Adam_weight_decay)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59be3230",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f882d7c5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a3d7dff4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "n_epochs = 100\n",
    "epoch_loss = []\n",
    "val_epoch_loss = []\n",
    "print(\"Starting Training!\")\n",
    "for epoch in range(0,n_epochs+1):\n",
    "    #print(\"epoch: \", epoch)\n",
    "    epoch_losses = []\n",
    "    for batch in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        batch_inds = batch.batch.to(device)\n",
    "      \n",
    "        # batch of graphs has edge attribs, node attribs - (n_nodes, n_features+1) -> concat (n_nodes, attrib1)\n",
    "\n",
    "        batch.x = batch.x.float()#.to(device)\n",
    "        #batch.edge_index = batch.edge_index.to(device)\n",
    "\n",
    "        # Barlow - get 2 random views of batch\n",
    "        b1 = (batch.x, batch.edge_index, batch.edge_attr)\n",
    "        b2 = (batch.x, batch.edge_index, batch.edge_attr)\n",
    "\n",
    "        # Embed each batch (ignoring representations)\n",
    "        r1, e1 = model(b1, batch_inds)\n",
    "        r2, e2 = model(b2, batch_inds)\n",
    "\n",
    "        loss = VicRegLoss(e1, e2)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        epoch_losses.append(loss.data.item())\n",
    "        \n",
    "    epoch_loss.append(sum(epoch_losses) / len(epoch_losses))\n",
    "    print('epoch', epoch,'train loss:', sum(epoch_losses) / len(epoch_losses))\n",
    "\n",
    "    \n",
    "    val_loss = 0\n",
    "    model.eval()\n",
    "    \n",
    "    # VicReg Validation Loss\n",
    "    val_epoch_losses = []\n",
    "    for batch in val_loader:\n",
    "        with torch.no_grad():\n",
    "            # VicReg validation loss\n",
    "            b1 = (batch.x, batch.edge_index, batch.edge_attr)\n",
    "            b2 = (batch.x, batch.edge_index, batch.edge_attr)\n",
    "            r1, e1 = model(b1, batch.batch.to(device))\n",
    "            r2, e2 = model(b2, batch.batch.to(device))\n",
    "                \n",
    "            val_loss = VicRegLoss(e1, e2)\n",
    "            val_epoch_losses.append(val_loss.data.item())\n",
    "            \n",
    "\n",
    "    val_epoch_loss.append(sum(val_epoch_losses) / len(val_epoch_losses))    \n",
    "    print('epoch', epoch,'val loss:', sum(val_epoch_losses) / len(val_epoch_losses))\n",
    "\n",
    "    \n",
    "print(\"Done training GCL model!\")        \n",
    "               "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a409dd59",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cd900cd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot train-val loss curves\n",
    "plot_loss_curves(epoch_loss, val_epoch_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef83d662",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4988c1a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94645624",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b40a27fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lgbm_models(x_train, x_test, y_train, y_test, target_feature, params, list_target_features):\n",
    "    lgb_train = lgb.Dataset(x_train.values, y_train[target_feature].values, params={'verbose': -1})\n",
    "    lgb_eval = lgb.Dataset(x_test.values, y_test[target_feature].values, reference=lgb_train, params={'verbose': -1})\n",
    "    gbm = lgb.train(params['params'],\n",
    "                    lgb_train,\n",
    "                    num_boost_round=params['num_boost_round'],\n",
    "                    valid_sets=lgb_eval,\n",
    "                    callbacks=params['callbacks'])\n",
    "    lgb_yhat = gbm.predict(x_test.values, num_iteration=gbm.best_iteration)\n",
    "    lgb_score = round(mean_squared_error(y_test[target_feature].values, lgb_yhat), 2)\n",
    "    print(\"LightGBM Model MSE for \", target_feature, \": \", lgb_score)\n",
    "\n",
    "    return lgb_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef4ac190",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0efdbb28",
   "metadata": {},
   "outputs": [],
   "source": [
    "#a function to run linear models\n",
    "def fit_models(x_tr, y_tr, rf_params, lgbm_params, lm_fit_dict, rf_fit_dict):\n",
    "    # For each task in QM9\n",
    "    \n",
    "    for (_, target_feature) in enumerate(y_tr):\n",
    "        if target_feature == 'index':\n",
    "            pass\n",
    "        else:\n",
    "            if not target_feature in lm_fit_dict: #dictionaries are empty   #first batch of data loader\n",
    "                lm_fit_dict[target_feature] = LinearRegression()\n",
    "                rf_fit_dict[target_feature] = RandomForestRegressor(n_estimators=rf_params['n_estimators'], max_depth=rf_params['max_depth'], warm_start=True)\n",
    "\n",
    "\n",
    "            ## Fit a model on model representation of train set:\n",
    "            ##Need to drop missing values for linear models, since they do not allow these\n",
    "            lm = lm_fit_dict[target_feature]\n",
    "            lm.fit(x_tr.values, y_tr[target_feature].values)\n",
    "            lm_fit_dict[target_feature] = lm\n",
    "        \n",
    "            #Fit Random Forest models here:            \n",
    "            rf = rf_fit_dict[target_feature]\n",
    "            rf.fit(x_tr.values, y_tr[target_feature].values)\n",
    "            rf.n_estimators += 2 #sklearn's RF might not work with batch stuff...\n",
    "            rf_fit_dict[target_feature] = rf\n",
    "            \n",
    "            \n",
    "\n",
    "\n",
    "        #Fit LightGBM models here (LightGBM is supposedly better than XGBoost):\n",
    "                    \n",
    "  \n",
    "    return lm_fit_dict, rf_fit_dict\n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d4e07a1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#a function to run linear models\n",
    "def predict_models(x_test, y_test, lm_fit_dict, rf_fit_dict, lm_results_dict, rf_results_dict, val_n):\n",
    "    # For each task in QM9\n",
    "\n",
    "    means_vector = y_test.mean(axis = 0)\n",
    "    rep_means_vectors = means_vector.repeat(x_test.shape[0]) #create a vector where each entry is the mean\n",
    "\n",
    "    for (_, target_feature) in enumerate(y_test):\n",
    "        if target_feature == 'index':\n",
    "            pass\n",
    "        else:\n",
    "            if not target_feature in lm_results_dict:\n",
    "                lm_results_dict[target_feature] = 0\n",
    "                rf_results_dict[target_feature] = 0\n",
    "                baseline_dict[target_feature] = 0\n",
    "       \n",
    "            ## Fit a model on model representation of train set:\n",
    "            ##Need to drop missing values for linear models, since they do not allow these\n",
    "\n",
    "            \n",
    "            lm = lm_fit_dict[target_feature]\n",
    "            lm_yhat = lm.predict(x_test.values)\n",
    "\n",
    "            \n",
    "            lm_score = mean_squared_error(y_test[target_feature], lm_yhat)*y_test[target_feature].shape[0]/val_n\n",
    "            #need to multiply by size of batch and divide by size of val set to end up with correct MSE\n",
    "            r2 = r2_score(y_test[target_feature].values, lm_yhat)\n",
    "            lm_results_dict[target_feature] += lm_score\n",
    "            #this cumulatively adds MSE\n",
    "\n",
    "            #Fit Random Forest models here:\n",
    "\n",
    "            rf = rf_fit_dict[target_feature]\n",
    "            rf_yhat = rf.predict(x_test.values)\n",
    "            rf_score = mean_squared_error(y_test[target_feature], rf_yhat)*y_test[target_feature].shape[0]/val_n\n",
    "            #need to multiply by size of batch and divide by size of val set to end up with correct MSE\n",
    "            rf_results_dict[target_feature] += rf_score\n",
    "\n",
    "\n",
    "            #baseline is a model that always outputs the mean of the training sample        \n",
    "            rep_means_vectors = means_vector[target_feature].repeat(x_test.shape[0])\n",
    "            baseline = mean_squared_error(y_test[target_feature].values, rep_means_vectors)*y_test[target_feature].shape[0]/val_n\n",
    "            #need to multiply by size of batch and divide by size of val set to end up with correct MSE\n",
    "            baseline_dict[target_feature] += baseline\n",
    "            #this cumulatively adds MSE\n",
    "    \n",
    "    return lm_results_dict, rf_results_dict, baseline_dict\n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e1edc526",
   "metadata": {},
   "outputs": [],
   "source": [
    "def qm9_preprocessing(x_df, y_df, qm9_index):\n",
    "    #need to drop the two rows with extremely large Rotational Constant A\n",
    "    y_df.rename(columns=qm9_index, inplace=True)\n",
    "    indexes = y_df.index[(y_df[\"Rotational_constant_A\"] > 100000)]\n",
    "    y_df.drop(y_df.index[indexes], axis=0, inplace=True)\n",
    "    x_df.drop(labels=indexes, axis=0, inplace=True)\n",
    "    y_df = y_df.reset_index(drop=True)\n",
    "    x_df = x_df.reset_index(drop=True)\n",
    "\n",
    "    return x_df, y_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8d99057e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_fit_params(fit_dict):\n",
    "    fit_params = {}\n",
    "    for (_, target_feature) in enumerate(fit_dict):\n",
    "        fit_params[target_feature] = fit_dict[target_feature].get_params\n",
    "    \n",
    "    return fit_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73b90f6b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b32018a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#get embeddings, train downstream models on embeddings\n",
    "\n",
    "x = pd.DataFrame()\n",
    "y = pd.DataFrame()\n",
    "#graph_chem_formulae_dictionaries = pd.DataFrame() #use for XenonPy transform\n",
    "\n",
    "lm_fit_dict = {}\n",
    "rf_fit_dict = {}\n",
    "print(\"Fitting models!\")\n",
    "for batch in train_loader: # take entire train set\n",
    "    #graph_chem_formulae_dictionaries = get_mol_dict(batch)  #use for XenonPy transform\n",
    "    with torch.no_grad():\n",
    "        # Embed training set under model\n",
    "        rep, _ = model((batch.x, batch.edge_index, batch.edge_attr), batch.batch.to(device))\n",
    "        if torch.cuda.is_available():\n",
    "            rep = rep.to(\"cpu\")\n",
    "    x_tr = pd.DataFrame(rep.numpy())\n",
    "    y_tr = pd.DataFrame(batch.y).astype(\"float\")\n",
    "    x_tr, y_tr = qm9_preprocessing(x_tr, y_tr, qm9_index) #preprocess data to get rid of outliers in Rotational Constant A\n",
    "\n",
    "    lm_fit_dict, rf_fit_dict = fit_models(x_tr, y_tr, rf_parameters, lgbm_params, lm_fit_dict, rf_fit_dict)\n",
    "print(\"Done fitting models!\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e4393209",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "lm_results_dict = {}\n",
    "rf_results_dict = {}\n",
    "baseline_dict = {}\n",
    "print(\"Starting predictions of validation set!\")\n",
    "for batch in val_loader: # take entire val set\n",
    "    #graph_chem_formulae_dictionaries = get_mol_dict(batch)  #use for XenonPy transform\n",
    "    with torch.no_grad():\n",
    "        # Embed training set under model\n",
    "        rep, _ = model((batch.x, batch.edge_index, batch.edge_attr), batch.batch.to(device))\n",
    "        if torch.cuda.is_available():\n",
    "            rep = rep.to(\"cpu\")\n",
    "    x_val = pd.DataFrame(rep.numpy())\n",
    "    y_val = pd.DataFrame(batch.y).astype(\"float\")\n",
    "    x_val, y_val = qm9_preprocessing(x_val, y_val, qm9_index)\n",
    "    lm_results_dict, rf_results_dict, baseline_dict = predict_models(x_val, y_val, lm_fit_dict, rf_fit_dict, lm_results_dict, rf_results_dict, val_n)\n",
    "print(\"Done predicting validation set!\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "b6f7209a",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_dict = {'LM_results':lm_results_dict, 'RF_results':rf_results_dict, 'Basic_model':baseline_dict}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "d3695c86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'LM_results': {'Dipole_moment': 2.0967168245895524, 'Isotropic_polarizability': 238.018302217651, 'HOMO': 0.32850993141395557, 'LUMO': 0.9064186811912672, 'HOMO_LUMO_gap': 1.0693878815994142, 'Electronic_spatial_extent': 193328.18038347532, 'Zero_point_vibrational_energy': 0.5875629788131156, 'Internal_energy_at_0K': 3286159.928007401, 'Internal_energy_at_298.15K': 3278477.8080382715, 'Enthalpy_at_298.15K': 3281296.1875931653, 'Free_energy_at_298.15K': 3291175.113975062, 'Heat_capacity_at_298.15K': 50.65492777177651, 'Atomization_energy_at_0K': 148.35932850761571, 'Atomization_energy_at_298.15K': 151.43838057207302, 'Atomization_enthalpy_at_298.15K': 152.70499525116145, 'Atomization_free_energy_at_298.15K': 126.65138389926327, 'Rotational_constant_A': 84.7447085400399, 'Rotational_constant_B': 26.416320421548846, 'Rotational_constant_C': 11.386504064943503}, 'RF_results': {'Dipole_moment': 1.3092370595871663, 'Isotropic_polarizability': 124.37906223245848, 'HOMO': 0.22958599983763484, 'LUMO': 0.5642081888432655, 'HOMO_LUMO_gap': 0.6861598589766086, 'Electronic_spatial_extent': 127094.98113341953, 'Zero_point_vibrational_energy': 0.4344733962869666, 'Internal_energy_at_0K': 2474187.8997643157, 'Internal_energy_at_298.15K': 2491708.831804325, 'Enthalpy_at_298.15K': 2487724.669025763, 'Free_energy_at_298.15K': 2474661.6293361345, 'Heat_capacity_at_298.15K': 24.521377900494983, 'Atomization_energy_at_0K': 126.45392908534942, 'Atomization_energy_at_298.15K': 127.41826574825551, 'Atomization_enthalpy_at_298.15K': 128.34396545684777, 'Atomization_free_energy_at_298.15K': 107.99065280343034, 'Rotational_constant_A': 88.44678281610294, 'Rotational_constant_B': 24.04911785130698, 'Rotational_constant_C': 10.407112780567278}, 'Basic_model': {'Dipole_moment': 2.114415277171156, 'Isotropic_polarizability': 78.37116869532822, 'HOMO': 0.45055569890668984, 'LUMO': 1.6216463437342983, 'HOMO_LUMO_gap': 1.6999902371103652, 'Electronic_spatial_extent': 59713.68690750381, 'Zero_point_vibrational_energy': 0.8101501936543158, 'Internal_energy_at_0K': 1339736.2436403416, 'Internal_energy_at_298.15K': 1339708.1931007658, 'Enthalpy_at_298.15K': 1339708.172012127, 'Free_energy_at_298.15K': 1339789.4993057717, 'Heat_capacity_at_298.15K': 19.934294119377963, 'Atomization_energy_at_0K': 114.89457718614412, 'Atomization_energy_at_298.15K': 116.80807104928168, 'Atomization_enthalpy_at_298.15K': 118.45986988754288, 'Atomization_free_energy_at_298.15K': 97.23242426421108, 'Rotational_constant_A': 90.49141823369828, 'Rotational_constant_B': 24.39480285078269, 'Rotational_constant_C': 10.490405195584165}}\n"
     ]
    }
   ],
   "source": [
    "print(results_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff03c7e7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "988ea597",
   "metadata": {},
   "outputs": [],
   "source": [
    "fit_params_dict = get_fit_params(rf_fit_dict) #gets fit parameters of RF models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11147931",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "59a20124",
   "metadata": {},
   "outputs": [],
   "source": [
    "run_test = False #only run this at the very, very, very end on the test set\n",
    "if run_test:\n",
    "    for batch in test_loader: # take entire test set\n",
    "        #graph_chem_formulae_dictionaries = get_mol_dict(batch)  #use for XenonPy transform\n",
    "        with torch.no_grad():\n",
    "            # Embed training set under model\n",
    "            rep, _ = model((batch.x, batch.edge_index, batch.edge_attr), batch.batch.to(device))\n",
    "            if torch.cuda.is_available():\n",
    "                rep = rep.to(\"cpu\")\n",
    "        x_test = pd.DataFrame(rep.numpy())\n",
    "        y_test = pd.DataFrame(batch.y).astype(\"float\")\n",
    "        x_test, y_test = qm9_preprocessing(x_test, y_test, qm9_index)\n",
    "        test_lm_results_dict, test_rf_results_dict, test_baseline_dict = predict_models(x_test, y_test, lm_fit_dict, rf_fit_dict, test_lm_results_dict, test_rf_results_dict, test_n)\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "071bdd33",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b9ec2250",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_XenonPy_transform(tr_graph_chem_dict, val_graph_chem_dict, test_graph_chem_dict, col_name, dataset):\n",
    "    #returns XenonPy transformations for tr, val, and test sets\n",
    "    print(\"XenonPy transformation starting for training set!\")\n",
    "    xenonpy_obj = XPy.XenonPy_transformation(tr_graph_chem_dict, col_name)\n",
    "    tr_df_XenonPy = xenonpy_obj.XenonPy_transform()\n",
    "    print(\"Done with training set!\")\n",
    "    print(\"XenonPy transformation starting for validation set!\")\n",
    "    xenonpy_obj = XPy.XenonPy_transformation(val_graph_chem_dict, col_name)\n",
    "    val_df_XenonPy = xenonpy_obj.XenonPy_transform()\n",
    "    print(\"Done with validation set!\")\n",
    "    print(\"XenonPy transformation starting for test set!\")\n",
    "    xenonpy_obj = XPy.XenonPy_transformation(test_graph_chem_dict, col_name)\n",
    "    test_df_XenonPy = xenonpy_obj.XenonPy_transform()\n",
    "    print(\"Done with test set!!!!\")\n",
    "    save_xenonpy_transformation(tr_df_XenonPy, val_df_XenonPy, test_df_XenonPy, dataset)\n",
    "    \n",
    "    return tr_df_XenonPy, val_df_XenonPy, test_df_XenonPy\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c9cd93d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_xenonpy_transformation(tr_df, val_df, test_df, dataset_name): #saves xenonpy transformation to folder\n",
    "    #only do this once per database!!\n",
    "    #filename should be the name of the database\n",
    "    parent_dir = '/home/ewvertina/Molecular_modelling'\n",
    "    new_folder = dataset_name\n",
    "    path = os.path.join(parent_dir, new_folder)\n",
    "    os.mkdir(path)\n",
    "    tr_path = path + '/xenon_tr.csv'\n",
    "    val_path = path + '/xenon_val.csv'\n",
    "    test_path = path + '/xenon_test.csv'\n",
    "    tr_df.to_csv(tr_path)  \n",
    "    val_df.to_csv(val_path)\n",
    "    test_df.to_csv(test_path)\n",
    "    return print(\"XenonPy transformations saved!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a7d94ee1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#get embeddings for training set\n",
    "#x_tr, y_tr, model = get_embeddings(train_loader, model, tr_aug)\n",
    "\n",
    "#x_tr, y_tr, tr_graph_chem_formulae_dictionaries, model = get_embeddings(train_loader, model, tr_aug)\n",
    "#use this for XenonPy transform\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e5b818d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#get embeddings for validation set\n",
    "#x_val, y_val, val_graph_chem_formulae_dictionaries, model = get_embeddings(val_loader, model, val_aug)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "949b5048",
   "metadata": {},
   "outputs": [],
   "source": [
    "#get embeddings for test set\n",
    "#x_test, y_test, test_graph_chem_formulae_dictionaries, model = get_embeddings(test_loader, model, test_aug)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c199ad45",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f40f1c5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "6de54630",
   "metadata": {},
   "outputs": [],
   "source": [
    "## transforms tr, val, and test sets with XenonPy and saves to local system\n",
    "#col_name = 'formula' #column with molecular dictionaries\n",
    "#tr_df_XenonPy, val_df_XenonPy, test_df_XenonPy = get_XenonPy_transform(tr_graph_chem_formulae_dictionaries, val_graph_chem_formulae_dictionaries, test_graph_chem_formulae_dictionaries, col_name, dataset)                    \n",
    "#tr_graph_chem_formulae_dictionaries, etc., contain dict of molecules (see XenonPy)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e99766bf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3177b64b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "5cde40c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#need to make sure that I am getting the correct graphs for y's"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "d9ccf5fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#join XenonPy transformations with x_tr and x_val\n",
    "#need to access file from saved location\n",
    "#x_tr.join(tr_df_XenonPy)\n",
    "#x_val.join(val_df_XenonPy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e39bc7ef",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "2e6fd975",
   "metadata": {},
   "outputs": [],
   "source": [
    "def df_extreme_values(df):\n",
    "    \n",
    "    for column in range(df.shape[1]):\n",
    "        sorted_index_array = np.argsort(df[column])\n",
    "        sorted_array = df[column][sorted_index_array]\n",
    "        n = 10\n",
    "\n",
    "        # find n largest value\n",
    "        max_rslt = sorted_array[-n : ]\n",
    "        min_rslt = sorted_array[ : n]\n",
    "        #print(rslt)\n",
    "        # show the output\n",
    "        print(qm9_index_list[column], \"max values:\\n\", max_rslt)\n",
    "        print(qm9_index_list[column], \"min values:\\n\", min_rslt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "050e7774",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba85539f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "99d75e5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#x_tr = pd.concat([x_tr, x_val]) #tr and val combined for training set\n",
    "#y_tr = pd.concat([y_tr, y_val]) #tr and val combined for test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "bbbf5b47",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#results_list, model_info = linear_models(x_tr, x_test, y_tr, y_test, qm9_index_list, rf_parameters, lgbm_parameters)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4911fc9f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "9e0d95f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#linear_models(x_tr_no_aug, x_val_no_aug, y_tr_no_aug, y_val_no_aug, qm9_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdc76a88",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5386ce4c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "96262e32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# record end time\n",
    "t_1 = timeit.default_timer()\n",
    " \n",
    "# calculate elapsed time\n",
    "elapsed_time = round((t_1 - t_0) , 1)\n",
    "print(f\"Elapsed time: {elapsed_time} seconds\")\n",
    "elapsed_time_minutes = round((elapsed_time/60), 2)\n",
    "print(f\"Elapsed time: {elapsed_time_minutes} minutes\")\n",
    "elapsed_time_hours = round((elapsed_time/3600), 2)\n",
    "print(f\"Elapsed time: {elapsed_time_hours} hours\")\n",
    "\n",
    "other_info = {'dataset':dataset, 'hours':elapsed_time_hours, 'minutes':elapsed_time_minutes, 'seconds':elapsed_time}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d020e298",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "eb70c3f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "run = True\n",
    "if run == True:\n",
    "    print(\"Saved!\")\n",
    "    #save experimental results\n",
    "    current_time = datetime.now()\n",
    "    dt_string = current_time.strftime(\"%Y-%m-%d_%H_%M\")\n",
    "    directory = dt_string\n",
    "    parent_dir = '/home/ewvertina/Molecular_modelling/Experiment_Results/'\n",
    "    path = os.path.join(parent_dir, directory)\n",
    "    os.mkdir(path)\n",
    "    path_state_dict = path + '/state_dict'\n",
    "    path_results_dict = path + '/results_dict.txt'\n",
    "    path_fit_params_dict = path + '/fit_params_dict'\n",
    "    path_runtime = path + '/runtime.txt'\n",
    "    path_parameters = path + '/parameters_used.txt'\n",
    "    path_fig = path + '/train_test_loss.png'\n",
    "    \n",
    "    #save NN model as a torch dictionary\n",
    "    torch.save(model.state_dict(), path_state_dict)\n",
    "    torch.save(results_dict, path_results_dict)\n",
    "    torch.save(fit_params_dict, path_fit_params_dict)\n",
    "    torch.save(other_info, path_runtime) #save which dataset, runtime\n",
    "    torch.save(parameters_used, path_parameters) #saves all parameters used\n",
    "    plt.savefig(path_fig) #save train-val loss figure\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "188f0640",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48c6280b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Hello, world!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66527be7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f65cb39f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "304940bb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23057d75",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
